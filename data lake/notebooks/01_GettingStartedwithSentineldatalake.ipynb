{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56308008",
   "metadata": {},
   "source": [
    "# üöÄ Getting Started with Apache Spark for Sentinel data lake\n",
    "\n",
    "This notebook is designed for **beginners** (security analysts, data engineers, SOC researchers) \n",
    "who are new to working with Spark notebooks and Microsoft Sentinel data.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "- üîó Connect to Sentinel data lake using the `MicrosoftSentinelProvider`\n",
    "- üìä Load and explore log data (SecurityEvent, SigninLogs, AuditLogs, etc.)\n",
    "- üîç Perform common **security analysis queries** using Spark operations\n",
    "- üíæ Save processed results back to the data lake for further analysis\n",
    "- üóëÔ∏è Safely manage tables (create, read, delete)\n",
    "\n",
    "> **üí° Tip**: This is a hands-on tutorial - run each cell step by step to see the results!\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Step 1: Connect to Sentinel data lake\n",
    "\n",
    "First, we'll establish a connection using the **MicrosoftSentinelProvider** - your gateway to Sentinel data.\n",
    "\n",
    "### What is MicrosoftSentinelProvider?\n",
    "The `MicrosoftSentinelProvider` is a Python class that:\n",
    "- üîó Connects your Spark session to Microsoft Sentinel's data lake\n",
    "- üìã Lists available databases and tables\n",
    "- üìñ Reads security log data at scale\n",
    "- üíæ Saves processed results back to the data lake\n",
    "\n",
    "### Getting Started\n",
    "‚û°Ô∏è Simply initialize the provider with your active Spark session - that's it!\n",
    "\n",
    "üìö **Learn More**: [Microsoft Sentinel Provider Class Reference](https://learn.microsoft.com/en-us/azure/sentinel/datalake/sentinel-provider-class-reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f919bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries for data manipulation and pyspark operations\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "# Loading the MicrosoftSentinelProvider from sentinel_lake library\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "print(\"‚úÖ MicrosoftSentinelProvider initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335cb4a1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Configure Parameters & Tables\n",
    "\n",
    "Before we dive into the data, let's set up our configuration parameters.\n",
    "\n",
    "### üìã What We're Configuring:\n",
    "- **üïí Time Window** ‚Üí How far back to look for data (e.g., last 1 hour)\n",
    "- **üìÇ Input Table** ‚Üí Which Sentinel table to read from (SigninLogs in our example)\n",
    "- **üéØ Output Table** ‚Üí Where to save our processed results\n",
    "- **üè¢ Workspace** ‚Üí Your Sentinel workspace name\n",
    "\n",
    "### üîß Why This Matters:\n",
    "- **Reusability**: Change dates/tables without modifying code\n",
    "- **Performance**: Smaller time windows = faster queries\n",
    "- **Organization**: Clear naming helps track your analysis\n",
    "\n",
    "> **‚ö†Ô∏è Remember**: Replace `<YOUR_WORKSPACE_NAME>` with your actual Sentinel workspace name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ed9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window\n",
    "lookback_hours = 1\n",
    "run_end = datetime.now().replace(minute=0, second=0, microsecond=0)  # Round down to the nearest hour\n",
    "run_start = run_end - timedelta(hours=lookback_hours)   # Setting start time based on lookback hours relative to run_end\n",
    "\n",
    "# Workspace name (replace with your own Sentinel workspace)\n",
    "workspace_name = \"<YOUR_WORKSPACE_NAME>\"\n",
    "\n",
    "# Table names\n",
    "input_table_raw = \"SigninLogs\"\n",
    "output_datalake_table = \"Test_output_table_SPRK\"\n",
    "\n",
    "# Write options (append keeps history, partition improves query performance)\n",
    "write_options = {\"mode\": \"append\"} # simple append mode options for demo\n",
    "\n",
    "print(\"üìÖ Time Window:\", run_start, \"‚Üí\", run_end)\n",
    "print(\"‚úÖ Parameters configured\")\n",
    "\n",
    "print(\"üìÇ Tables configured as:\")\n",
    "print(f\"\\t   Input:      {input_table_raw}\")\n",
    "print(f\"\\t   Output Table: {output_datalake_table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739443b6",
   "metadata": {},
   "source": [
    "## üìä Step 3: Load & Transform Data\n",
    "\n",
    "Now we'll load real security data from the **SigninLogs** table and prepare it for analysis.\n",
    "\n",
    "### üéØ What This Code Does:\n",
    "1. **üìñ Reads Data**: Connects to SigninLogs in your Analytics Tier workspace\n",
    "2. **üîç Filters Time**: Only gets data from our specified time window\n",
    "3. **üèóÔ∏è Transforms Data**: \n",
    "   - Selects key security fields (user, IP, location, etc.)\n",
    "   - Expands JSON location data into separate columns (City, Country, coordinates)\n",
    "   - Adds a date column for easier querying\n",
    "4. **‚ö° Optimizes**: Caches the data in memory for faster subsequent operations\n",
    "\n",
    "### üóÇÔ∏è Key Fields We're Working With:\n",
    "- **Identity**: UserPrincipalName, UserDisplayName\n",
    "- **Network**: IPAddress, AutonomousSystemNumber (ASN)\n",
    "- **Location**: City, Country, Latitude/Longitude (from LocationDetails JSON)\n",
    "- **Security**: ResultType, ResultSignature, Status\n",
    "- **Context**: UserAgent, TimeGenerated\n",
    "\n",
    "> **üí° Pro Tip**: The `.cache()` operation stores data in memory, making repeated queries much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core fields + enrichment\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    \"LocationDetails\",      # JSON: city, state, countryOrRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\"\n",
    "]\n",
    "\n",
    "df_recent_raw = (\n",
    "    data_provider.read_table(input_table_raw, workspace_name)   # Read from SigninLogs table from Analytics tier - workspace_name is provided\n",
    "    .select(*signin_fields)\n",
    "    .filter((F.col(\"TimeGenerated\") > F.lit(run_start)) & \n",
    "            (F.col(\"TimeGenerated\") <= F.lit(run_end))) # Filter by time window of last 1 hour\n",
    "    .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "    # Expand JSON fields\n",
    "    .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "    .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "    .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "    .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "    .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "df_recent_raw.printSchema()\n",
    "print(\"‚úÖ Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe29ff8",
   "metadata": {},
   "source": [
    "## üîç Step 4: Explore Security Data\n",
    "\n",
    "Now for the fun part - let's analyze the data! We'll demonstrate common security analysis patterns.\n",
    "\n",
    "### üéØ Example Analysis: Failed Login Attempts\n",
    "\n",
    "We're looking for accounts with failed authentication attempts (ResultType = \"50126\").\n",
    "This helps identify:\n",
    "- üö® **Potential brute force attacks**\n",
    "- üîê **Accounts under attack**\n",
    "- üìä **Attack patterns and trends**\n",
    "\n",
    "### üõ†Ô∏è What the Code Does:\n",
    "1. **Filters** for failed login attempts\n",
    "2. **Groups** by user account\n",
    "3. **Counts** failures per account\n",
    "4. **Sorts** by highest failure count\n",
    "5. **Displays** top 10 targeted accounts\n",
    "\n",
    "> **üí° Security Insight**: High failure counts on specific accounts often indicate targeted attacks or compromised credentials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22796d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"‚è≥ Preparing to show DataFrame...Groupby-Count-Orderby Operations may take few minutes ‚åõ\")\n",
    "display(            # display will show nicely formatted HTML table in notebook . Alternatively, use .show() for text table output\n",
    "    df_recent_raw.filter(F.col(\"ResultType\") == \"50126\")\n",
    "             .groupBy(\"UserPrincipalName\")\n",
    "             .count()\n",
    "             .orderBy(F.desc(\"count\")).limit(10)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Displayed Example: Top 10 accounts with failed logons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de51c8",
   "metadata": {},
   "source": [
    "## üíæ Step 5: Save Results to data lake\n",
    "\n",
    "After processing and analyzing data, you'll often want to save results for:\n",
    "- üìä **Dashboards and reports**\n",
    "- üîç **Future investigations** \n",
    "- ü§ù **Sharing with team members**\n",
    "- ‚ö° **Faster subsequent queries**\n",
    "\n",
    "### üéØ What We're Doing:\n",
    "Using `save_as_table()` to write our processed data back to the Sentinel data lake.\n",
    "\n",
    "### üìã Save Options:\n",
    "- **Table Name**: `Test_output_table_SPRK` (clearly marked as test data)\n",
    "- **Mode**: `append` (adds to existing data rather than overwriting)\n",
    "- **Tier**: \"System tables\" (data lake tier for long-term storage)\n",
    "\n",
    "> **‚ö†Ô∏è Important**: This creates a test table that we'll clean up at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4910085",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_provider.save_as_table(\n",
    "            df_recent_raw,\n",
    "            output_datalake_table,\n",
    "            \"System tables\",        # System tables refers to writing to data lake tier table.\n",
    "            write_options\n",
    "        )\n",
    "    print(f\"‚úÖ Wrote test data into {output_datalake_table}\")\n",
    "except Exception as save_err:\n",
    "    print(f\"‚ùå Failed writing data into {output_datalake_table}: {save_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429921",
   "metadata": {},
   "source": [
    "## üóëÔ∏è Step 6: Clean Up Test Data\n",
    "\n",
    "### ‚ö†Ô∏è **CAUTION: Table Deletion**\n",
    "\n",
    "We're about to demonstrate the `delete_table()` operation. This is **permanent** and **cannot be undone**.\n",
    "\n",
    "### üéØ Why We're Doing This:\n",
    "- üßπ **Clean up**: Remove the test table we created\n",
    "- üìö **Learning**: Show you how to safely manage tables\n",
    "- \udcb0 **Cost control**: Avoid unnecessary storage charges\n",
    "\n",
    "### üõ°Ô∏è Safety Guidelines:\n",
    "- ‚úÖ **Only delete tables you created for testing**\n",
    "- ‚ùå **Never delete production tables without team approval**\n",
    "- üìù **Always double-check the table name before deletion**\n",
    "\n",
    "> **\udca1 Best Practice**: In production, implement approval workflows and backup procedures before any deletion operations!\n",
    "\n",
    "### üîç What's Being Deleted:\n",
    "Table: `Test_output_table_SPRK` (our demo table from Step 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86531f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚ö†Ô∏è Deleting table - {output_datalake_table} - created for demo purposes\")\n",
    "data_provider.delete_table(output_datalake_table) \n",
    "\n",
    "print(\"‚úÖ Specified Table deleted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99246578",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1fdbde2",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed your first Spark notebook for Microsoft Sentinel data lake analysis! \n",
    "\n",
    "### üöÄ What You've Accomplished:\n",
    "- ‚úÖ Connected to Sentinel data lake using MicrosoftSentinelProvider\n",
    "- ‚úÖ Loaded and transformed real security data (SigninLogs)\n",
    "- ‚úÖ Performed security analysis (failed login detection)\n",
    "- ‚úÖ Saved results back to the data lake\n",
    "- ‚úÖ Safely managed table operations (create/delete)\n",
    "\n",
    "### üéØ Next Steps:\n",
    "- **Explore More Tables**: Try SecurityEvent, AuditLogs, or DeviceEvents\n",
    "- **Advanced Analytics**: Implement time-series analysis and anomaly detection\n",
    "- **Automation**: Schedule notebooks to run automatically\n",
    "- **Visualization**: Create dashboards from your saved results\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References & Further Learning\n",
    "\n",
    "### üîó Microsoft Sentinel data lake Documentation:\n",
    "- **[Sentinel Provider Class Reference](https://learn.microsoft.com/en-us/azure/sentinel/datalake/sentinel-provider-class-reference)** - Complete API documentation for MicrosoftSentinelProvider\n",
    "- **[Notebook Jobs in Sentinel](https://learn.microsoft.com/en-us/azure/sentinel/datalake/notebook-jobs)** - Learn to schedule and automate your notebooks\n",
    "\n",
    "### üìñ Additional Learning Resources:\n",
    "- **[Apache Spark Official Documentation](https://spark.apache.org/docs/latest/)** - Comprehensive Spark programming guide\n",
    "- **[PySpark Quickstart: DataFrame](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)**  - Introduction and quickstart for the PySpark DataFrame API\n",
    "- **[Microsoft Sentinel Skill-up Training](https://learn.microsoft.com/en-us/azure/sentinel/skill-up-resources)** - Free training modules for security analysts\n",
    "\n",
    "### üõ†Ô∏è Advanced Topics:\n",
    "- **[PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)** - Master data manipulation techniques\n",
    "- **[Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)** - Powerful built-in functions for data analysis\n",
    "\n",
    "### ü§ù Community & Support:\n",
    "- **[Microsoft Sentinel data lake GitHub](https://github.com/microsoft/Sentinel)** - Out-of-the Box Notebooks and KQL queries for data lake.\n",
    "- **[Microsoft Tech Community](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)** - Connect with other security professionals\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Pro Tip**: Use this notebook as a starter template for your own security analysis projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small pool (4 vCores) [01_GettingStartedwithSentineldatalake]",
   "language": "Python",
   "name": "MSGSmall"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
