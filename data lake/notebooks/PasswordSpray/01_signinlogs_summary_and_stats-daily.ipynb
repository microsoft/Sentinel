{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe053d6",
   "metadata": {},
   "source": [
    "# ğŸ” SigninLogs Summary + Stats Daily Notebook\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“– Overview\n",
    "\n",
    "This notebook produces **two key tables** from raw Microsoft Entra ID `SigninLogs`:Â  \n",
    "\n",
    "- ğŸ§® **signinlogs_summary_daily_SPRK** â†’ pre-aggregated per-IP features per day (efficient base for rolling 30-day candidate analysis).Â  \n",
    "\n",
    "- ğŸ“Š **signin_stats_daily_SPRK** â†’ daily rollups for reporting, dashboards, and baselines.Â  \n",
    "\n",
    "By combining both in a single run, we avoid scanning raw logs twice, saving cost and runtime.Â  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "\n",
    "- ğŸ¯ Load raw `SigninLogs` with relevant fields (user, IP, ASN, geo, status).\n",
    "- ğŸ”§ Expand JSON location details into structured columns (City, Country, Latitude, Longitude).\n",
    "- ğŸ“Š Compute **rolling window features** per IP: attempts, distinct users, days active, entropy.\n",
    "- ğŸ·ï¸ Calculate a **spray score** and assign **labels** (LOW / MEDIUM / HIGH).\n",
    "- ğŸ’¾ Write results to dedicated Sentinel data lake tables for further use (detections, dashboards, investigations).\n",
    "- ğŸ‘€ Provide preview of schema and sample rows for validation.\n",
    "- ğŸ›¡ï¸ Include safeguards for table deletion in case of incorrect backfill.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ğŸ—ºï¸ Data Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "   A[ğŸ“¥ SigninLogs<br/>Raw Data] --> A1[ğŸ“‘ Column Selection<br/>TimeGenerated, User, IP, ASN, Location, Result*]\n",
    "   \n",
    "   %% Daily Summary\n",
    "   A1 --> B[ğŸ§® Daily Summary per IP<br/>by IP, ASN, City, Country, Date]\n",
    "   B -->|Aggregates| B1[ğŸ”¢ attempts_total, success_count,<br/>distinct_users, first_seen, last_seen,<br/>username_entropy]\n",
    "   B1 --> D[ğŸ’¾ signin_summary_daily_SPRK<br/>partitioned by date]\n",
    "\n",
    "   %% Daily Stats\n",
    "   A1 --> C[ğŸ“Š Daily Stats Rollup<br/>by Date only]\n",
    "   C -->|Aggregates| C1[ğŸ”¢ total_attempts, distinct_users,<br/>distinct_IPs, lockouts, successes]\n",
    "   C1 --> E[ğŸ’¾ signin_stats_daily_SPRK<br/>partitioned by date]\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Dynamic Time window Parameters for a daily recurrent run.\n",
    "end_date = datetime.now().date() - timedelta(days=1)  # yesterday\n",
    "start_date = end_date   # Same as end_date for daily runs as inclusive of both start and end while filtering.\n",
    "\n",
    "# Sentinel Workspace name (update for your environment)\n",
    "workspace_name = \"<YOUR_WORKSPACE_NAME>\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table_raw = \"SigninLogs\"\n",
    "output_data_lake_summary_table = \"signin_summary_daily_SPRK\"\n",
    "output_data_lake_stats_table = \"signin_stats_daily_SPRK\"\n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\", \"partitionBy\": [\"date\"]}   # partitionBy needs to be a list of columns and only supported in data lake tier.\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment - natively available in SigninLogs\n",
    "    \"LocationDetails\",  # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"ğŸ“… Data Loading Parameters\")\n",
    "print(f\"ğŸ‘‰   Start â†’ End: {start_date} â†’ {end_date}\")\n",
    "\n",
    "print(\"\\n\\nâš ï¸ Please validate input Tables and target tables before proceeding:\")\n",
    "print(f\"\\tğŸ“‚  Input Raw Table:      {input_table_raw}\")\n",
    "print(f\"\\tğŸ“‚  Signins Summary: {output_data_lake_summary_table}\")\n",
    "print(f\"\\tğŸ“‚  Signins Stats: {output_data_lake_stats_table}\")\n",
    "\n",
    "print(\"\\n\\nğŸ“ Selected Fields:\")\n",
    "print(\"ğŸ‘‰   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74093f",
   "metadata": {},
   "source": [
    "# ğŸ“¥ Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Microsoft Entra ID `SigninLogs` table.  \n",
    "\n",
    "- ğŸ“Œ **Purpose**: Focus only on fields relevant to password spray detection.  \n",
    "- ğŸŒ **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).  \n",
    "- ğŸ”¢ **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.  \n",
    "- ğŸ“… **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.  \n",
    "\n",
    "âœ… At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_daily_df = (\n",
    "    data_provider.read_table(input_table_raw, workspace_name)\n",
    "        .select(*signin_fields)\n",
    "        .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "        .filter((F.col(\"TimeGenerated\") >= F.lit(start_date)) &\n",
    "                (F.col(\"TimeGenerated\") <= F.lit(end_date)))  # end date and start date are inclusive to load data for single day.\n",
    "        .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "        .withColumn(\"State\", F.get_json_object(\"LocationDetails\", \"$.state\"))\n",
    "        .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "        .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "        .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "        .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    "        .cache()        # Cache as used multiple times below\n",
    ")\n",
    "\n",
    "\n",
    "signin_daily_df.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"âœ… Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef4b3f",
   "metadata": {},
   "source": [
    "# ğŸ§® Daily Summary per IP\n",
    "\n",
    "This step aggregates authentication activity **per source IP, per day**, providing richer attribution context than global daily stats.  \n",
    "It compresses raw sign-in logs into **per-IP rollups** while still retaining critical behavioral signals.\n",
    "\n",
    "---\n",
    "### ğŸ”„ Transformation Details\n",
    "For each combination of `IPAddress`, `ASN`, `City`, `Country`, and `date`, we compute:\n",
    "\n",
    "- ğŸ”¢ **attempts_total** â†’ total number of sign-in attempts from that IP on that day  \n",
    "- âœ… **success_count** â†’ number of successful logons from the IP  \n",
    "- ğŸ‘¤ **distinct_users** â†’ number of unique targeted accounts  \n",
    "- â±ï¸ **first_seen / last_seen** â†’ earliest and latest timestamps for that IPâ€™s activity on the day  \n",
    "\n",
    "Additionally, we enrich with **username entropy**:  \n",
    "- ğŸ§® **username_entropy** â†’ measures the randomness/spread of targeted usernames.  \n",
    "   - Low entropy = focused attack (few accounts repeatedly).  \n",
    "   - High entropy = broad spray (many accounts targeted).  \n",
    "\n",
    "---\n",
    "### ğŸ’¾ Output\n",
    "The results are appended into the **daily summary table** (`signin_summary_daily_SPRK`), partitioned by `date`.  \n",
    "This table enables:  \n",
    "- âš¡ Efficient long lookbacks (no need to scan raw `SigninLogs`)  \n",
    "- ğŸ¯ Foundation for **features scoring** (spray likelihood models)  \n",
    "- ğŸ” Investigative pivots by IP, ASN, or geographic location  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_all = (\n",
    "    signin_daily_df.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\", \"date\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"attempts_total\"),\n",
    "            F.sum(F.when(F.col(\"ResultSignature\") == \"Success\", 1).otherwise(0)).alias(\"success_count\"),\n",
    "            F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "            F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "            F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "        )\n",
    ")\n",
    "\n",
    "user_counts = signin_daily_df.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "entropy = (\n",
    "    user_counts\n",
    "        .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "        .groupBy(\"IPAddress\")\n",
    "        .agg(F.round(-F.sum(F.col(\"p\") * F.log2(\"p\")), 2).alias(\"username_entropy\"))\n",
    ")\n",
    "\n",
    "summary = agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "\n",
    "try:\n",
    "    data_provider.save_as_table(\n",
    "           summary,\n",
    "            output_data_lake_summary_table,\n",
    "            \"System tables\",        # System tables referes to writing to Datalake tier table.\n",
    "            write_options\n",
    "        )\n",
    "    print(f\"âœ… Wrote summary for {end_date} into {output_data_lake_summary_table}\")\n",
    "except Exception as save_err:\n",
    "    print(f\"âŒ Failed writing summary for {end_date}: {save_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e54f75",
   "metadata": {},
   "source": [
    "# ğŸ“Š Daily Summary Stats\n",
    "\n",
    "This step aggregates **all sign-in activity for a given day** into a compact rollup table.  \n",
    "Instead of tracking activity per IP, this view focuses on **global daily metrics** that help analysts understand the overall scale and impact of password spray attempts.\n",
    "\n",
    "---\n",
    "### ğŸ”„ Transformation Details\n",
    "For each `date`, we compute:\n",
    "\n",
    "- ğŸ”¢ **total_attempts** â†’ total number of sign-in attempts observed  \n",
    "- ğŸ‘¤ **distinct_targeted_users** â†’ number of unique accounts targeted that day  \n",
    "- ğŸŒ **distinct_source_ips** â†’ number of unique source IPs seen attempting logons  \n",
    "- ğŸš« **lockouts** â†’ count of lockout errors (`ResultType=50053`), indicating repeated failed attempts  \n",
    "- âœ… **successes** â†’ count of successful logons, useful for spotting when a spray attack succeeds  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¾ Output\n",
    "The aggregated results are appended into the **daily stats table** (`signin_stats_daily_SPRK`), partitioned by `date`.  \n",
    "This table enables:  \n",
    "- ğŸ“ˆ High-level trend dashboards  \n",
    "- ğŸš¨ Alert thresholds (e.g., sudden spikes in distinct IPs or lockouts)  \n",
    "- ğŸ” Context for investigations when reviewing candidate IP behavior  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (\n",
    "    signin_daily_df.groupBy(\"date\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_attempts\"),\n",
    "            F.countDistinct(\"UserPrincipalName\").alias(\"distinct_targeted_users\"),\n",
    "            F.countDistinct(\"IPAddress\").alias(\"distinct_source_ips\"),\n",
    "            F.sum(F.when(F.col(\"ResultType\") == \"50053\", 1).otherwise(0)).alias(\"lockouts\"),\n",
    "            F.sum(F.when(F.col(\"ResultSignature\") == \"Success\", 1).otherwise(0)).alias(\"successes\")\n",
    "        )\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_provider.save_as_table(\n",
    "            stats,\n",
    "            output_data_lake_stats_table,\n",
    "            \"System tables\",        # System tables referes to writing to Datalake tier table.\n",
    "            write_options\n",
    "        )\n",
    "    print(f\"âœ… Wrote daily stats for {end_date} into {output_data_lake_stats_table}\")\n",
    "except Exception as save_err:\n",
    "    print(f\"âŒ Failed writing stats for {end_date}: {save_err}\")\n",
    "\n",
    "# Release cached DataFrame\n",
    "signin_daily_df.unpersist()\n",
    "print(\"ğŸ§¹ Released cached signin_daily_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f22b08",
   "metadata": {},
   "source": [
    "# ğŸ‘€ Preview Outputs\n",
    "\n",
    "After running the backfill loop, itâ€™s important to validate that the pipeline produced the expected data.  \n",
    "This section shows **schemas** and **sample rows** for both output tables.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—‚ï¸ Daily Summary Table (`signin_summary_daily_SPRK`)\n",
    "\n",
    "This table provides **per-IP, per-day aggregates** of sign-in activity.  \n",
    "It compresses raw `SigninLogs` into a daily rollup for each source IP, while preserving attribution context.  \n",
    "\n",
    "- ğŸŒ **Geo & ASN context** â†’ IPAddress, ASN, City, Country  \n",
    "- ğŸ“… **Date** â†’ reporting day of the aggregation  \n",
    "- ğŸ”¢ **Total attempts** â†’ total number of authentication attempts from the IP on that day  \n",
    "- âœ… **Success count** â†’ number of successful logons (helps measure spray effectiveness)  \n",
    "- ğŸ‘¤ **Distinct users** â†’ number of unique targeted accounts  \n",
    "- â±ï¸ **First seen / Last seen** â†’ earliest and latest attempt timestamps for that IP within the day  \n",
    "- ğŸ§® **Username entropy** â†’ entropy score measuring spread/randomness of targeted usernames  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Daily Stats Table (`signin_stats_daily_SPRK`)\n",
    "\n",
    "This table provides **daily rollups** of authentication activity.  \n",
    "It helps track the overall level of spray attempts and anomalies over time.\n",
    "\n",
    "- ğŸ“… **Date** â†’ reporting date  \n",
    "- ğŸ”¢ **Total attempts** â†’ all sign-in attempts for the day  \n",
    "- ğŸ‘¤ **Distinct targeted users** â†’ how many unique accounts were hit  \n",
    "- ğŸŒ **Distinct source IPs** â†’ how many unique IPs attempted logons  \n",
    "- ğŸš« **Lockouts** â†’ counts of account lockout events (ResultType=50053)  \n",
    "- âœ… **Successes** â†’ counts of successful logons  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Preview sample outputs\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nğŸ“‘ Daily Summary Table Schema\")\n",
    "summary.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Daily Summary Sample Rows from the loaded spark dataframe\")\n",
    "print(\"â³ Preparing to show DataFrame...this may take 2â€“3 minutes âŒ›\")\n",
    "display(\n",
    "    summary.select(\n",
    "        \"date\", \"IPAddress\", \"attempts_total\", \"success_count\",\n",
    "        \"distinct_users\", \"username_entropy\", \"ASN\", \"City\", \"Country\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‘ Daily Stats Table Schema\")\n",
    "stats.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Daily Stats Sample Rows from the loaded spark dataframe\")\n",
    "print(\"â³ Preparing to show DataFrame...this may take 2â€“3 minutes âŒ›\")\n",
    "display(\n",
    "    stats.select(\n",
    "        \"date\", \"total_attempts\", \"distinct_targeted_users\",\n",
    "        \"distinct_source_ips\", \"lockouts\", \"successes\"\n",
    "    ).limit(10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [01_signinlogs_summary_and_stats-daily]",
   "language": "Python",
   "name": "MSGMedium"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
