{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe053d6",
   "metadata": {},
   "source": [
    "# 🔐 SigninLogs Summary + Stats Daily Notebook\n",
    "\n",
    "\n",
    "\n",
    "## 📖 Overview\n",
    "\n",
    "This notebook produces **two key tables** from raw Microsoft Entra ID `SigninLogs`:  \n",
    "\n",
    "- 🧮 **signinlogs_summary_daily_SPRK** → pre-aggregated per-IP features per day (efficient base for rolling 30-day candidate analysis).  \n",
    "\n",
    "- 📊 **signin_stats_daily_SPRK** → daily rollups for reporting, dashboards, and baselines.  \n",
    "\n",
    "By combining both in a single run, we avoid scanning raw logs twice, saving cost and runtime.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Objectives\n",
    "\n",
    "- 🎯 Load raw `SigninLogs` with relevant fields (user, IP, ASN, geo, status).\n",
    "- 🔧 Expand JSON location details into structured columns (City, Country, Latitude, Longitude).\n",
    "- 📊 Compute **rolling window features** per IP: attempts, distinct users, days active, entropy.\n",
    "- 🏷️ Calculate a **spray score** and assign **labels** (LOW / MEDIUM / HIGH).\n",
    "- 💾 Write results to dedicated Sentinel data lake tables for further use (detections, dashboards, investigations).\n",
    "- 👀 Provide preview of schema and sample rows for validation.\n",
    "- 🛡️ Include safeguards for table deletion in case of incorrect backfill.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 🗺️ Data Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "   A[📥 SigninLogs<br/>Raw Data] --> A1[📑 Column Selection<br/>TimeGenerated, User, IP, ASN, Location, Result*]\n",
    "   \n",
    "   %% Daily Summary\n",
    "   A1 --> B[🧮 Daily Summary per IP<br/>by IP, ASN, City, Country, Date]\n",
    "   B -->|Aggregates| B1[🔢 attempts_total, success_count,<br/>distinct_users, first_seen, last_seen,<br/>username_entropy]\n",
    "   B1 --> D[💾 signin_summary_daily_SPRK<br/>partitioned by date]\n",
    "\n",
    "   %% Daily Stats\n",
    "   A1 --> C[📊 Daily Stats Rollup<br/>by Date only]\n",
    "   C -->|Aggregates| C1[🔢 total_attempts, distinct_users,<br/>distinct_IPs, lockouts, successes]\n",
    "   C1 --> E[💾 signin_stats_daily_SPRK<br/>partitioned by date]\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Dynamic Time window Parameters for a daily recurrent run.\n",
    "end_date = datetime.now().date() - timedelta(days=1)  # yesterday\n",
    "start_date = end_date   # Same as end_date for daily runs as inclusive of both start and end while filtering.\n",
    "\n",
    "# Sentinel Workspace name (update for your environment)\n",
    "workspace_name = \"<YOUR_WORKSPACE_NAME>\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table_raw = \"SigninLogs\"\n",
    "output_datalake_summary_table = \"signin_summary_daily_SPRK\"\n",
    "output_datalake_stats_table = \"signin_stats_daily_SPRK\"\n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\", \"partitionBy\": [\"date\"]}   # partitionBy needs to be a list of columns and only supported in data lake tier.\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment - natively available in SigninLogs\n",
    "    \"LocationDetails\",  # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"📅 Data Loading Parameters\")\n",
    "print(f\"👉   Start → End: {start_date} → {end_date}\")\n",
    "\n",
    "print(\"\\n\\n⚠️ Please validate input Tables and target tables before proceeding:\")\n",
    "print(f\"\\t📂  Input Raw Table:      {input_table_raw}\")\n",
    "print(f\"\\t📂  Signins Summary: {output_datalake_summary_table}\")\n",
    "print(f\"\\t📂  Signins Stats: {output_datalake_stats_table}\")\n",
    "\n",
    "print(\"\\n\\n📝 Selected Fields:\")\n",
    "print(\"👉   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74093f",
   "metadata": {},
   "source": [
    "# 📥 Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Microsoft Entra ID `SigninLogs` table.  \n",
    "\n",
    "- 📌 **Purpose**: Focus only on fields relevant to password spray detection.  \n",
    "- 🌍 **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).  \n",
    "- 🔢 **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.  \n",
    "- 📅 **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.  \n",
    "\n",
    "✅ At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_daily_df = (\n",
    "    data_provider.read_table(input_table_raw, workspace_name)\n",
    "        .select(*signin_fields)\n",
    "        .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "        .filter((F.col(\"TimeGenerated\") >= F.lit(start_date)) &\n",
    "                (F.col(\"TimeGenerated\") <= F.lit(end_date)))  # end date and start date are inclusive to load data for single day.\n",
    "        .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "        .withColumn(\"State\", F.get_json_object(\"LocationDetails\", \"$.state\"))\n",
    "        .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "        .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "        .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "        .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    "        .cache()        # Cache as used multiple times below\n",
    ")\n",
    "\n",
    "\n",
    "signin_daily_df.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"✅ Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef4b3f",
   "metadata": {},
   "source": [
    "# 🧮 Daily Summary per IP\n",
    "\n",
    "This step aggregates authentication activity **per source IP, per day**, providing richer attribution context than global daily stats.  \n",
    "It compresses raw sign-in logs into **per-IP rollups** while still retaining critical behavioral signals.\n",
    "\n",
    "---\n",
    "### 🔄 Transformation Details\n",
    "For each combination of `IPAddress`, `ASN`, `City`, `Country`, and `date`, we compute:\n",
    "\n",
    "- 🔢 **attempts_total** → total number of sign-in attempts from that IP on that day  \n",
    "- ✅ **success_count** → number of successful logons from the IP  \n",
    "- 👤 **distinct_users** → number of unique targeted accounts  \n",
    "- ⏱️ **first_seen / last_seen** → earliest and latest timestamps for that IP’s activity on the day  \n",
    "\n",
    "Additionally, we enrich with **username entropy**:  \n",
    "- 🧮 **username_entropy** → measures the randomness/spread of targeted usernames.  \n",
    "   - Low entropy = focused attack (few accounts repeatedly).  \n",
    "   - High entropy = broad spray (many accounts targeted).  \n",
    "\n",
    "---\n",
    "### 💾 Output\n",
    "The results are appended into the **daily summary table** (`signin_summary_daily_SPRK`), partitioned by `date`.  \n",
    "This table enables:  \n",
    "- ⚡ Efficient long lookbacks (no need to scan raw `SigninLogs`)  \n",
    "- 🎯 Foundation for **features scoring** (spray likelihood models)  \n",
    "- 🔍 Investigative pivots by IP, ASN, or geographic location  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_all = (\n",
    "    signin_daily_df.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\", \"date\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"attempts_total\"),\n",
    "            F.sum(F.when(F.col(\"ResultSignature\") == \"Success\", 1).otherwise(0)).alias(\"success_count\"),\n",
    "            F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "            F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "            F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "        )\n",
    ")\n",
    "\n",
    "user_counts = signin_daily_df.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "entropy = (\n",
    "    user_counts\n",
    "        .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "        .groupBy(\"IPAddress\")\n",
    "        .agg(F.round(-F.sum(F.col(\"p\") * F.log2(\"p\")), 2).alias(\"username_entropy\"))\n",
    ")\n",
    "\n",
    "summary = agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "\n",
    "try:\n",
    "    data_provider.save_as_table(\n",
    "           summary,\n",
    "            output_datalake_summary_table,\n",
    "            \"System tables\",        # System tables referes to writing to Datalake tier table.\n",
    "            write_options\n",
    "        )\n",
    "    print(f\"✅ Wrote summary for {end_date} into {output_datalake_summary_table}\")\n",
    "except Exception as save_err:\n",
    "    print(f\"❌ Failed writing summary for {end_date}: {save_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e54f75",
   "metadata": {},
   "source": [
    "# 📊 Daily Summary Stats\n",
    "\n",
    "This step aggregates **all sign-in activity for a given day** into a compact rollup table.  \n",
    "Instead of tracking activity per IP, this view focuses on **global daily metrics** that help analysts understand the overall scale and impact of password spray attempts.\n",
    "\n",
    "---\n",
    "### 🔄 Transformation Details\n",
    "For each `date`, we compute:\n",
    "\n",
    "- 🔢 **total_attempts** → total number of sign-in attempts observed  \n",
    "- 👤 **distinct_targeted_users** → number of unique accounts targeted that day  \n",
    "- 🌐 **distinct_source_ips** → number of unique source IPs seen attempting logons  \n",
    "- 🚫 **lockouts** → count of lockout errors (`ResultType=50053`), indicating repeated failed attempts  \n",
    "- ✅ **successes** → count of successful logons, useful for spotting when a spray attack succeeds  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 💾 Output\n",
    "The aggregated results are appended into the **daily stats table** (`signin_stats_daily_SPRK`), partitioned by `date`.  \n",
    "This table enables:  \n",
    "- 📈 High-level trend dashboards  \n",
    "- 🚨 Alert thresholds (e.g., sudden spikes in distinct IPs or lockouts)  \n",
    "- 🔍 Context for investigations when reviewing candidate IP behavior  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (\n",
    "    signin_daily_df.groupBy(\"date\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_attempts\"),\n",
    "            F.countDistinct(\"UserPrincipalName\").alias(\"distinct_targeted_users\"),\n",
    "            F.countDistinct(\"IPAddress\").alias(\"distinct_source_ips\"),\n",
    "            F.sum(F.when(F.col(\"ResultType\") == \"50053\", 1).otherwise(0)).alias(\"lockouts\"),\n",
    "            F.sum(F.when(F.col(\"ResultSignature\") == \"Success\", 1).otherwise(0)).alias(\"successes\")\n",
    "        )\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_provider.save_as_table(\n",
    "            stats,\n",
    "            output_datalake_stats_table,\n",
    "            \"System tables\",        # System tables referes to writing to Datalake tier table.\n",
    "            write_options\n",
    "        )\n",
    "    print(f\"✅ Wrote daily stats for {end_date} into {output_datalake_stats_table}\")\n",
    "except Exception as save_err:\n",
    "    print(f\"❌ Failed writing stats for {end_date}: {save_err}\")\n",
    "\n",
    "# Release cached DataFrame\n",
    "signin_daily_df.unpersist()\n",
    "print(\"🧹 Released cached signin_daily_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f22b08",
   "metadata": {},
   "source": [
    "# 👀 Preview Outputs\n",
    "\n",
    "After running the backfill loop, it’s important to validate that the pipeline produced the expected data.  \n",
    "This section shows **schemas** and **sample rows** for both output tables.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂️ Daily Summary Table (`signin_summary_daily_SPRK`)\n",
    "\n",
    "This table provides **per-IP, per-day aggregates** of sign-in activity.  \n",
    "It compresses raw `SigninLogs` into a daily rollup for each source IP, while preserving attribution context.  \n",
    "\n",
    "- 🌍 **Geo & ASN context** → IPAddress, ASN, City, Country  \n",
    "- 📅 **Date** → reporting day of the aggregation  \n",
    "- 🔢 **Total attempts** → total number of authentication attempts from the IP on that day  \n",
    "- ✅ **Success count** → number of successful logons (helps measure spray effectiveness)  \n",
    "- 👤 **Distinct users** → number of unique targeted accounts  \n",
    "- ⏱️ **First seen / Last seen** → earliest and latest attempt timestamps for that IP within the day  \n",
    "- 🧮 **Username entropy** → entropy score measuring spread/randomness of targeted usernames  \n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Daily Stats Table (`signin_stats_daily_SPRK`)\n",
    "\n",
    "This table provides **daily rollups** of authentication activity.  \n",
    "It helps track the overall level of spray attempts and anomalies over time.\n",
    "\n",
    "- 📅 **Date** → reporting date  \n",
    "- 🔢 **Total attempts** → all sign-in attempts for the day  \n",
    "- 👤 **Distinct targeted users** → how many unique accounts were hit  \n",
    "- 🌐 **Distinct source IPs** → how many unique IPs attempted logons  \n",
    "- 🚫 **Lockouts** → counts of account lockout events (ResultType=50053)  \n",
    "- ✅ **Successes** → counts of successful logons  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Preview sample outputs\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\n📑 Daily Summary Table Schema\")\n",
    "summary.printSchema()\n",
    "\n",
    "print(\"\\n🔍 Daily Summary Sample Rows from the loaded spark dataframe\")\n",
    "print(\"⏳ Preparing to show DataFrame...this may take 2–3 minutes ⌛\")\n",
    "display(\n",
    "    summary.select(\n",
    "        \"date\", \"IPAddress\", \"attempts_total\", \"success_count\",\n",
    "        \"distinct_users\", \"username_entropy\", \"ASN\", \"City\", \"Country\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\n📑 Daily Stats Table Schema\")\n",
    "stats.printSchema()\n",
    "\n",
    "print(\"\\n🔍 Daily Stats Sample Rows from the loaded spark dataframe\")\n",
    "print(\"⏳ Preparing to show DataFrame...this may take 2–3 minutes ⌛\")\n",
    "display(\n",
    "    stats.select(\n",
    "        \"date\", \"total_attempts\", \"distinct_targeted_users\",\n",
    "        \"distinct_source_ips\", \"lockouts\", \"successes\"\n",
    "    ).limit(10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [01_signinlogs_summary_and_stats-daily]",
   "language": "Python",
   "name": "MSGMedium"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
