{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b90d51",
   "metadata": {},
   "source": [
    "# ğŸ” [Optional] Password Spray Backfill Notebook\n",
    "\n",
    "## ğŸ“– Overview\n",
    "\n",
    "This notebook is optional and is designed to **backfill historical data** from Microsoft Entra ID `SigninLogs` into two derived tables:\n",
    "\n",
    "- ğŸ§© **Features Table** â†’ Per-IP behavioral features with spray score and labels.\n",
    "- ğŸ“Š **Daily Stats Table** â†’ Rollup metrics of sign-in activity per day.\n",
    "\n",
    "It provides both **long lookback detection** (to uncover low-and-slow password spray campaigns) and **daily snapshots** for monitoring trends.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "\n",
    "- ğŸ¯ Load raw `SigninLogs` with relevant fields (user, IP, ASN, geo, status).\n",
    "- ğŸ”§ Expand JSON location details into structured columns (City, Country, Latitude, Longitude).\n",
    "- ğŸ“Š Compute **rolling window features** per IP: attempts, distinct users, days active, entropy.\n",
    "- ğŸ·ï¸ Calculate a **spray score** and assign **labels** (LOW / MEDIUM / HIGH).\n",
    "- ğŸ’¾ Write results to dedicated Sentinel data lake tables for further use (detections, dashboards, investigations).\n",
    "- ğŸ‘€ Provide preview of schema and sample rows for validation.\n",
    "- ğŸ›¡ï¸ Include safeguards for table deletion in case of incorrect backfill.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Workflow\n",
    "\n",
    "1. ğŸ”§ **Parameters & Config** â€” define date ranges, table names, and selected fields.\n",
    "2. ğŸ“¥ **Load Raw SigninLogs** â€” read source logs, expand location details, and prepare base DataFrame.\n",
    "3. ğŸ”„ **Backfill Loop** â€” iterate day by day, compute candidate features + daily stats, save to Sentinel data lake.\n",
    "4. ğŸ“Š **Preview Outputs** â€” inspect schemas and sample rows for both output tables.\n",
    "5. âš ï¸ **Optional Reset** â€” safeguard-protected section to delete tables if a re-run is required.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Data Flow (Mermaid Diagram)\n",
    "\n",
    "````mermaid\n",
    "flowchart TD\n",
    "    A[ğŸ“¥ SigninLogs - Raw Data] --> B[ğŸ” Preprocessing<br/>Select Fields + Expand JSON]\n",
    "    B --> C[ğŸ§® Candidate Features<br/> Attempts, Users, Entropy, Geo, ASN]\n",
    "    B --> D[ğŸ“Š Daily Stats<br/> Totals, Distinct IPs/Users, Lockouts, Successes]\n",
    "\n",
    "    C --> E[ğŸ’¾ password_spray_features_SPRK<br/>Table]\n",
    "    D --> F[ğŸ’¾ signin_stats_daily_SPRK<br/>Table]\n",
    "\n",
    "    E --> G[ğŸ“ˆ Dashboards & Reports]\n",
    "    F --> G\n",
    "    E --> H[ğŸš¨ Alerts]\n",
    "    F --> H\n",
    "    E --> I[ğŸ•µï¸ Investigations]\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6ceac",
   "metadata": {},
   "source": [
    "# ğŸ”§ Parameters & Config\n",
    "\n",
    "This section defines the **runtime parameters** and ensures the notebook can be easily reused or adapted.\n",
    "\n",
    "- ğŸ“… **Date ranges** â€” specify the `backfill_start_date` and `backfill_end_date` along with the rolling `lookback_window_days`.\n",
    "- ğŸ—‚ï¸ **Table names** â€” input (`SigninLogs`) and output (`password_spray_features_SPRK`, `signin_stats_daily_SPRK`) tables are set here for easy swapping.\n",
    "- ğŸ“ **Selected fields** â€” only the most relevant fields are read (User, IP, ASN, Location, Status, etc.), reducing processing overhead.\n",
    "- âš™ï¸ **Write options** â€” define how results are written (append mode preserves history).\n",
    "\n",
    "This modular setup means analysts can quickly adjust **dates, fields, or destinations** without touching the core logic.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8825922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Lookback range (e.g., last <lookback_days> days up to yesterday)\n",
    "lookback_days = 30\n",
    "backfill_end_date = datetime.now().date() - timedelta(days=1)  # yesterday\n",
    "backfill_start_date = backfill_end_date - timedelta(days=lookback_days - 1)\n",
    "\n",
    "# Sentinel Workspace name (update for your environment)\n",
    "# workspace_name = \"<YourWorkspaceName>\"  # Replace with your actual workspace name\n",
    "workspace_name = \"Woodgrove-LogAnalyiticsWorkspace\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table = \"SigninLogs\"\n",
    "output_datalake_summary_table = \"signin_summary_daily_SPRK\"\n",
    "output_datalake_stats_table = \"signin_stats_daily_SPRK\"\n",
    "output_datalake_features_table = \"password_spray_features_SPRK\"\n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\", \"partitionBy\": [\"date\"]}   # partitionBy needs to be a list of columns and only supported in data lake tier.\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment - natively available in SigninLogs\n",
    "    \"LocationDetails\",  # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\",\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"ğŸ“… Backfill Parameters\")\n",
    "print(f\"   Start â†’ End: {backfill_start_date} â†’ {backfill_end_date}\")\n",
    "print(f\"   Lookback:   {lookback_days} days\\n\")\n",
    "\n",
    "print(\"ğŸ“‚ Tables\")\n",
    "print(f\"   Input:      {input_table}\")\n",
    "print(f\"   Signins Summary: {output_datalake_summary_table}\")\n",
    "print(f\"   Signins Stats: {output_datalake_stats_table}\")\n",
    "print(f\"   Password Spray Features:      {output_datalake_features_table}\\n\")\n",
    "\n",
    "print(\"ğŸ“ Selected Fields:\")\n",
    "print(\"   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62f7a6",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Microsoft Entra ID `SigninLogs` table.\n",
    "\n",
    "- ğŸ“Œ **Purpose**: Focus only on fields relevant to password spray detection.\n",
    "- ğŸŒ **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).\n",
    "- ğŸ”¢ **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.\n",
    "- ğŸ“… **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.\n",
    "\n",
    "âœ… At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_df = (\n",
    "    data_provider.read_table(input_table, workspace_name)\n",
    "        .select(*signin_fields)\n",
    "        .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "        .filter((F.col(\"TimeGenerated\") >= F.lit(backfill_start_date)) &\n",
    "                (F.col(\"TimeGenerated\") <  F.lit(backfill_end_date + timedelta(days=1))))\n",
    "        # Expand LocationDetails JSON\n",
    "        .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "        .withColumn(\"State\", F.get_json_object(\"LocationDetails\", \"$.state\"))\n",
    "        .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "        .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "        .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "        # Rename ASN for consistency\n",
    "        .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    ")\n",
    "\n",
    "signin_df.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"âœ… Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9858ab",
   "metadata": {},
   "source": [
    "## ğŸ”„ Backfill Loop  \n",
    "\n",
    "The heart of the notebook â€” iterating from `backfill_start_date` â†’ `backfill_end_date`.  \n",
    "For each day, we build **two sets of outputs**: _daily per-IP summary_ and _daily stats rollup_.  \n",
    "\n",
    "### Step-by-Step  \n",
    "\n",
    "1. ğŸ“… **Define day window**  \n",
    "   - Midnight â†’ next midnight (UTC).  \n",
    "   - Ensures we process **full daily slices** consistently.  \n",
    "\n",
    "2. ğŸ“¥ **Load SigninLogs for the day**  \n",
    "   - Select required fields (User, IP, ASN, Location, Status).  \n",
    "   - Expand JSON `LocationDetails` into City, Country, Latitude, Longitude.  \n",
    "\n",
    "3. ğŸ§® **Compute daily per-IP summary**  \n",
    "   - Aggregate per IP: total attempts, distinct users, success count.  \n",
    "   - Track first_seen and last_seen timestamps.  \n",
    "   - Compute **username entropy** to measure spread of targeted accounts.  \n",
    "   - Produces **rich per-IP features** but without spray scoring.  \n",
    "\n",
    "4. ğŸ’¾ **Append to `signin_summary_daily`**  \n",
    "   - Partitioned by `date`.  \n",
    "   - Acts as a **feature history table**, powering later spray detection with long lookbacks.  \n",
    "\n",
    "5. ğŸ“Š **Compute daily stats rollup**  \n",
    "   - Aggregate per day:  \n",
    "     - total attempts  \n",
    "     - distinct targeted users  \n",
    "     - distinct source IPs  \n",
    "     - account lockouts (`ResultType=50053`)  \n",
    "     - successes (`ResultSignature=Success`).  \n",
    "\n",
    "6. ğŸ’¾ **Append to `signin_stats_daily`**  \n",
    "   - Partitioned by `date`.  \n",
    "   - Provides high-level visibility into attack volume and trends.  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“Œ Together, these outputs create a **foundation layer**:  \n",
    "- `signin_summary_daily` â†’ granular per-IP history for spray feature engineering.  \n",
    "- `signin_stats_daily` â†’ strategic daily KPIs for dashboards and reporting.  \n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph BackfillLoop[Backfill Loop - Daily Run]\n",
    "        A[ğŸ“… Current day = start_date] --> B[â±ï¸ Define Day Window<br/>midnight â†’ next midnight]\n",
    "        B --> C[ğŸ“¥ Load SigninLogs for the Day<br/>+ Expand Geo & ASN fields]\n",
    "\n",
    "        %% Daily Summary per IP\n",
    "        C --> D1[ğŸ§® Aggregate per IP<br/>attempts, distinct_users, first_seen, last_seen]\n",
    "        C --> D2[ğŸ”¢ Compute Username Entropy<br/>distribution spread]\n",
    "        D1 --> E1[ğŸ“Š Join Features per IP]\n",
    "        D2 --> E1\n",
    "        E1 --> F1[ğŸ’¾ Write to signin_summary_daily<br/>partitioned by date]\n",
    "\n",
    "        %% Daily Stats Rollup\n",
    "        C --> D3[ğŸ“Š Aggregate Daily Stats<br/>total attempts, distinct users, IPs, lockouts, successes]\n",
    "        D3 --> F2[ğŸ’¾ Write to signin_stats_daily<br/>partitioned by date]\n",
    "\n",
    "        %% Loop\n",
    "        F1 --> G[â¡ï¸ Increment Day â†’ Next]\n",
    "        F2 --> G\n",
    "        G -->|Repeat until end_date| A\n",
    "    end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backfill Loop (Reverse Order)\n",
    "# -----------------------------\n",
    "print(\"âš ï¸ Initiating backfill loop...Depending on the lookback range and data volume, this may take a while (3-10 mins per day).\")\n",
    "print(f\" Backfilling the data range (reverse): {backfill_end_date} â†’ {backfill_start_date}\")\n",
    "\n",
    "current = backfill_end_date\n",
    "while current >= backfill_start_date:\n",
    "    try:\n",
    "        # Define day window (midnight â†’ next midnight UTC)\n",
    "        run_start = datetime(current.year, current.month, current.day)  # midnight start\n",
    "        run_end   = run_start + timedelta(days=1)                       # midnight next day\n",
    "\n",
    "        print(f\"â–¶ï¸ Processing {current} â†’ {run_start} to {run_end}\")\n",
    "\n",
    "        # Load raw logs for this day\n",
    "        df_day = (\n",
    "            data_provider.read_table(input_table, workspace_name)\n",
    "                .select(*signin_fields)\n",
    "                .filter((F.col(\"TimeGenerated\") >= F.lit(run_start)) &\n",
    "                        (F.col(\"TimeGenerated\") <  F.lit(run_end)))\n",
    "                .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "                .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "                .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "                .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "                .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "                .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    "                .cache()\n",
    "        )\n",
    "\n",
    "        # ------------------\n",
    "        # Daily summary per IP\n",
    "        # ------------------\n",
    "        agg_all = (\n",
    "            df_day.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\", \"date\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"attempts_total\"),\n",
    "                    F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "                    F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "                    F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "                    F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        user_counts = df_day.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "        entropy = (\n",
    "            user_counts\n",
    "                .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "                .groupBy(\"IPAddress\")\n",
    "                .agg(F.round(-F.sum(F.col(\"p\") * F.log2(\"p\")), 2).alias(\"username_entropy\"))\n",
    "        )\n",
    "\n",
    "        summary = agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "\n",
    "        try:\n",
    "            data_provider.save_as_table(\n",
    "                summary,\n",
    "                output_datalake_summary_table,\n",
    "                \"System tables\",        # System tables referes to writing to Datalake tier table.\n",
    "                write_options\n",
    "            )\n",
    "            print(f\"âœ… Wrote summary for {current}\")\n",
    "        except Exception as save_err:\n",
    "            print(f\"âŒ Failed writing summary for {current}: {save_err}\")\n",
    "\n",
    "        # ------------------\n",
    "        # Daily stats rollup\n",
    "        # ------------------\n",
    "        stats = (\n",
    "            df_day.groupBy(\"date\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"total_attempts\"),\n",
    "                    F.countDistinct(\"UserPrincipalName\").alias(\"distinct_targeted_users\"),\n",
    "                    F.countDistinct(\"IPAddress\").alias(\"distinct_source_ips\"),\n",
    "                    F.sum(F.when(F.col(\"ResultType\")==\"50053\",1).otherwise(0)).alias(\"lockouts\"),\n",
    "                    F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"successes\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            data_provider.save_as_table(\n",
    "                stats,\n",
    "                output_datalake_stats_table,\n",
    "                \"System tables\",\n",
    "                write_options\n",
    "            )\n",
    "            print(f\"âœ… Wrote stats for {current}\")\n",
    "        except Exception as save_err:\n",
    "            print(f\"âŒ Failed writing stats for {current}: {save_err}\")\n",
    "\n",
    "        # Clean up\n",
    "        df_day.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {current}: {e}\")\n",
    "\n",
    "    # Move to previous day (reverse loop)\n",
    "    current -= timedelta(days=1)\n",
    "\n",
    "print(\"ğŸ‰ Backfill complete (reverse order)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a025798",
   "metadata": {},
   "source": [
    "## ğŸ”„ Features Backfill (Long Lookback + Daily Fresh)\n",
    "\n",
    "ğŸ‘‰ The **next step** is the **features table**, which consumes `signin_summary_daily` over a historical lookback (e.g., 30â€“90 days) to compute **spray_score** and labels.  \n",
    "\n",
    "This loop walks from `backfill_start_date` â†’ `backfill_end_date`.  \n",
    "For **each day `current`**, we compute **30-day lookback features** using:\n",
    "- **Historical** per-IP daily rollups from `signin_summary_daily` (compact + cost-efficient), and\n",
    "- **Fresh** daily raw from `SigninLogs` (the current dayâ€™s high-fidelity events).\n",
    "\n",
    "### ğŸ§­ Inputs\n",
    "- `SigninLogs` (current day slice)\n",
    "- `signin_summary_daily` (rolling window, e.g., `current - 30d â†’ current`)\n",
    "- Parameters: `lookback_days`, `backfill_start_date`, `backfill_end_date`\n",
    "\n",
    "### ğŸ“¦ Output (per iteration)\n",
    "- `password_spray_features` (partitioned by `run_date = current`)\n",
    "  - Features: `attempts_total`, `success_count`, `distinct_users`, `days_active`,\n",
    "    `first_seen`, `last_seen`, `username_entropy`\n",
    "  - Normalized: `distinct_users_norm`, `success_rate`, `entropy_norm`\n",
    "  - Score: `spray_score` and `spray_score_label` (LOW | MEDIUM | HIGH)\n",
    "  - Attribution: `IPAddress`, `ASN`, `City`, `Country`\n",
    "  - Metadata: `run_date`, `detection_window_start`, `detection_window_end`\n",
    "\n",
    "---\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph BackfillLoop[Backfill Loop - Daily Run]\n",
    "        A[ğŸ“… Current day = backfill_start_date] --> B[ğŸªŸ Define Lookback Window<br/>day - 30 days â†’ day]\n",
    "\n",
    "        %% Two parallel inputs\n",
    "        B --> C1[ğŸ“¥ Load signin_summary_daily<br/>Historical Lookback Data]\n",
    "        B --> C2[ğŸ“¥ Load signinlogs Daily<br/>Fresh Daily Data]\n",
    "\n",
    "        %% Merge inputs\n",
    "        C1 --> D[ğŸ”— Combine History + Recent Data]\n",
    "        C2 --> D\n",
    "\n",
    "        %% Feature engineering\n",
    "        D --> E[ğŸ§® Build Candidate Features<br/>Attempts, Users, Entropy, Successes]\n",
    "        E --> F[âš–ï¸ Normalize & Score<br/>spray_score, label]\n",
    "        F --> G[ğŸ’¾ Write to password_spray_features<br/>partition run_date = day]\n",
    "\n",
    "        %% Loop control\n",
    "        G --> H[â¡ï¸ Increment Day â†’ Next]\n",
    "        H -->|Repeat until backfill_end_date| A\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backfill Loop (Reverse)\n",
    "# -----------------------------\n",
    "print(\"âš ï¸ Initiating backfill loop in REVERSE order... Make sure the summary table is populated first with historical data to cover the window lookback.\")\n",
    "print(\"âš ï¸ Depending on the lookback range and data volume, this may take 3 to 5 mins per day.\")\n",
    "print(f\" Backfilling the data range: {backfill_start_date} â†’ {backfill_end_date}\")\n",
    "\n",
    "current = backfill_end_date\n",
    "while current >= backfill_start_date:\n",
    "    try:\n",
    "        window_start = current - timedelta(days=lookback_days - 1)\n",
    "        window_end   = current\n",
    "        print(f\"â–¶ï¸ Processing {current} | Lookback Window: {window_start} â†’ {window_end}\")\n",
    "\n",
    "        # -----------------\n",
    "        # Filter once & cache\n",
    "        # -----------------\n",
    "        df_window = (\n",
    "            signin_df.filter(\n",
    "                (F.col(\"date\") >= F.lit(window_start.isoformat())) &\n",
    "                (F.col(\"date\") <= F.lit(window_end.isoformat()))\n",
    "            )\n",
    "            .cache()\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Candidate features\n",
    "        # -----------------\n",
    "        agg_all = (\n",
    "            df_window.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\")\n",
    "              .agg(\n",
    "                F.count(\"*\").alias(\"attempts_total\"),\n",
    "                F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "                F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "                F.countDistinct(\"date\").alias(\"days_active\"),\n",
    "                F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "                F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "              )\n",
    "        )\n",
    "\n",
    "        # Entropy\n",
    "        user_counts = df_window.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "        entropy = (\n",
    "            user_counts\n",
    "              .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "              .groupBy(\"IPAddress\")\n",
    "              .agg((-F.sum(F.col(\"p\")*F.log2(\"p\"))).alias(\"username_entropy\"))\n",
    "        )\n",
    "\n",
    "        features = (\n",
    "            agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "                   .withColumn(\"run_date\", F.lit(current.isoformat()))\n",
    "                   .withColumn(\"detection_window_start\", F.lit(window_start.isoformat()))\n",
    "                   .withColumn(\"detection_window_end\", F.lit(window_end.isoformat()))\n",
    "        )\n",
    "\n",
    "        # Normalization (global max once)\n",
    "        if 'max_users' not in globals() or 'max_entropy' not in globals():\n",
    "            global_max = features.agg(\n",
    "                F.max(\"distinct_users\").alias(\"max_users\"),\n",
    "                F.round(F.max(\"username_entropy\"), 2).alias(\"max_entropy\")\n",
    "            ).first()\n",
    "\n",
    "            max_users = global_max.max_users or 1\n",
    "            max_entropy = global_max.max_entropy or 1.0\n",
    "\n",
    "        features = (\n",
    "            features\n",
    "            .withColumn(\"distinct_users_norm\", F.round(F.col(\"distinct_users\") / F.lit(max_users), 2))\n",
    "            .withColumn(\"success_rate\", F.round(F.col(\"success_count\") / F.greatest(F.col(\"attempts_total\"), F.lit(1)), 2))\n",
    "            .withColumn(\"entropy_norm\", F.round(F.col(\"username_entropy\") / F.lit(max_entropy), 2))\n",
    "            .withColumn(\n",
    "                \"spray_score\",\n",
    "                F.round(\n",
    "                    0.5*F.col(\"distinct_users_norm\") +\n",
    "                    0.2*(1 - F.col(\"success_rate\")) +\n",
    "                    0.3*F.col(\"entropy_norm\"),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"spray_score_label\",\n",
    "                F.when(F.col(\"spray_score\") < 0.3, F.lit(\"LOW\"))\n",
    "                 .when(F.col(\"spray_score\") < 0.6, F.lit(\"MEDIUM\"))\n",
    "                 .otherwise(F.lit(\"HIGH\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Safe write block\n",
    "        # -----------------\n",
    "        try:\n",
    "            write_options = {\"mode\": \"append\", \"partitionBy\": [\"run_date\"]}\n",
    "            data_provider.save_as_table(features, output_datalake_features_table, write_options=write_options)\n",
    "            print(f\"âœ… Wrote candidates for {current}\")\n",
    "        except Exception as write_err:\n",
    "            print(f\"âš ï¸ Skipped writing {current} due to error: {str(write_err)}\")\n",
    "\n",
    "        # Clean up\n",
    "        df_window.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {current}: {str(e)}\")\n",
    "\n",
    "    # Step backward in time\n",
    "    current -= timedelta(days=1)\n",
    "\n",
    "print(\"ğŸ‰ Reverse Backfill complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26220e44",
   "metadata": {},
   "source": [
    "# ğŸ‘€ Preview Outputs\n",
    "\n",
    "After running the backfill loop, itâ€™s important to validate that the pipeline produced the expected data.  \n",
    "This section shows **schemas** and **sample rows** for both output tables.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—‚ï¸ Daily Summary Table (`signin_summary_daily_SPRK`)\n",
    "\n",
    "This table provides **per-IP, per-day aggregates** of sign-in activity.  \n",
    "It compresses raw `SigninLogs` into a daily rollup for each source IP, while preserving attribution context.\n",
    "\n",
    "- ğŸŒ **Geo & ASN context** â†’ IPAddress, ASN, City, Country\n",
    "- ğŸ“… **Date** â†’ reporting day of the aggregation\n",
    "- ğŸ”¢ **Total attempts** â†’ total number of authentication attempts from the IP on that day\n",
    "- âœ… **Success count** â†’ number of successful logons (helps measure spray effectiveness)\n",
    "- ğŸ‘¤ **Distinct users** â†’ number of unique targeted accounts\n",
    "- â±ï¸ **First seen / Last seen** â†’ earliest and latest attempt timestamps for that IP within the day\n",
    "- ğŸ§® **Username entropy** â†’ entropy score measuring spread/randomness of targeted usernames\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Daily Stats Table (`signin_stats_daily_SPRK`)\n",
    "\n",
    "This table provides **daily rollups** of authentication activity.  \n",
    "It helps track the overall level of spray attempts and anomalies over time.\n",
    "\n",
    "- ğŸ“… **Date** â†’ reporting date\n",
    "- ğŸ”¢ **Total attempts** â†’ all sign-in attempts for the day\n",
    "- ğŸ‘¤ **Distinct targeted users** â†’ how many unique accounts were hit\n",
    "- ğŸŒ **Distinct source IPs** â†’ how many unique IPs attempted logons\n",
    "- ğŸš« **Lockouts** â†’ counts of account lockout events (ResultType=50053)\n",
    "- âœ… **Successes** â†’ counts of successful logons\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© Features Table (`password_spray_features_SPRK`)\n",
    "\n",
    "This table contains **per-IP aggregated features** across the rolling lookback window.  \n",
    "It helps identify which IP addresses exhibit password spray-like behavior.\n",
    "\n",
    "- ğŸŒ **Geo & ASN context** â†’ IPAddress, ASN, City, Country\n",
    "- ğŸ“Š **Behavioral metrics** â†’ attempts_total, distinct_users, days_active, entropy\n",
    "- ğŸ”¢ **Normalized features** â†’ distinct_users_norm, entropy_norm, success_rate\n",
    "- ğŸ¯ **Spray score** â†’ weighted score reflecting spray likelihood\n",
    "- ğŸ·ï¸ **Spray score label** â†’ LOW / MEDIUM / HIGH for easier triage\n",
    "\n",
    "---\n",
    "\n",
    "These outputs form the foundation for:\n",
    "\n",
    "- ğŸ“ˆ **Dashboards** (geo heatmaps, trend lines, score distributions)\n",
    "- ğŸš¨ **Alerts** (triggering on high-score IPs)\n",
    "- ğŸ•µï¸ **Investigations** (pivoting into ASN, city, or recurring IPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Preview sample outputs\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nğŸ“‘ Daily Summary Table Schema\")\n",
    "summary.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Daily Summary Sample Rows from the loaded spark dataframe\")\n",
    "display(\n",
    "    summary.select(\n",
    "        \"date\", \"IPAddress\", \"attempts_total\", \"success_count\",\n",
    "        \"distinct_users\", \"username_entropy\", \"ASN\", \"City\", \"Country\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‘ Daily Stats Table Schema\")\n",
    "stats.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Daily Stats Sample Rows from the loaded spark dataframe\")\n",
    "display(\n",
    "    stats.select(\n",
    "        \"date\", \"total_attempts\", \"distinct_targeted_users\",\n",
    "        \"distinct_source_ips\", \"lockouts\", \"successes\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‘ Candidates Table Schema\")\n",
    "features.printSchema()   # Prints schema only (no scan)\n",
    "\n",
    "print(\"\\nğŸ” Candidates Sample Rows from the loaded spark dataframe\")\n",
    "display(\n",
    "    features.select(\n",
    "        \"IPAddress\", \"ASN\", \"City\", \"Country\",\n",
    "        \"attempts_total\", \"distinct_users\", \"days_active\",\n",
    "        \"username_entropy\", \"spray_score\", \"spray_score_label\"\n",
    "    ).limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930087a5",
   "metadata": {},
   "source": [
    "# âš ï¸ Delete entire Sentinel data lake tables\n",
    "\n",
    "### Proceed with Caution\n",
    "\n",
    "Occasionally you may need to **reset** the output tables if the backfill wrote incorrect data.  \n",
    "Use this section **with extreme caution**:\n",
    "\n",
    "- ğŸš¨ This will **permanently delete** the table from the Sentinel data lake.\n",
    "- ğŸ”’ Keep the command **commented out by default**.\n",
    "- âœ… Only **uncomment** if you really intend to wipe and rebuild the table.\n",
    "\n",
    "Recommended workflow:\n",
    "\n",
    "1. Double-check table names (`features` vs `stats`) before deletion.\n",
    "2. Run deletion only in a controlled/test environment.\n",
    "3. Immediately rerun the notebook to regenerate fresh data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c457be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Uncomment the relevant block AND set confirm_delete = True to enable deletion\n",
    "\n",
    "confirm_delete = False   # Change to True only if you want to proceed\n",
    "\n",
    "if confirm_delete:\n",
    "    # print(f\"âš ï¸ Deleting table - {output_datalake_summary_table}\")\n",
    "    # data_provider.delete_table(output_datalake_summary_table)  # Delete candidate table\n",
    "\n",
    "    # print(f\"âš ï¸ Deleting table - {output_datalake_stats_table}\")\n",
    "    # data_provider.delete_table(output_datalake_stats_table)     # Delete daily stats table\n",
    "\n",
    "    # print(f\"âš ï¸ Deleting table - {output_datalake_features_table}\")\n",
    "    # data_provider.delete_table(output_datalake_features_table)  # Delete password spray features table\n",
    "\n",
    "    print(\"âš ï¸ Tables deleted. You must rerun the backfill to regenerate data.\")\n",
    "else:\n",
    "    print(\"âœ… Delete set to False, Safeguard active. No tables were deleted or action needed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small pool (4 vCores) [Optional-00_data_backfill_setup]",
   "language": "Python",
   "name": "MSGSmall"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
