{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b90d51",
   "metadata": {},
   "source": [
    "# ğŸ” Password Spray Backfill Notebook\n",
    "\n",
    "## ğŸ“– Overview\n",
    "This notebook is designed to **backfill historical data** from Azure AD `SigninLogs` into two derived tables:  \n",
    "- ğŸ§© **Features Table** â†’ Per-IP behavioral features with spray score and labels.  \n",
    "- ğŸ“Š **Daily Stats Table** â†’ Rollup metrics of sign-in activity per day.  \n",
    "\n",
    "It provides both **long lookback detection** (to uncover low-and-slow password spray campaigns) and **daily snapshots** for monitoring trends.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "- âœ… Load raw `SigninLogs` with relevant fields (user, IP, ASN, geo, status).  \n",
    "- âœ… Expand JSON location details into structured columns (City, Country, Latitude, Longitude).  \n",
    "- âœ… Compute **rolling window features** per IP: attempts, distinct users, days active, entropy.  \n",
    "- âœ… Calculate a **spray score** and assign **labels** (LOW / MEDIUM / HIGH).  \n",
    "- âœ… Write results to dedicated datalake tables for further use (detections, dashboards, investigations).  \n",
    "- âœ… Provide preview of schema and sample rows for validation.  \n",
    "- âœ… Include safeguards for table deletion in case of incorrect backfill.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Workflow\n",
    "1. ğŸ”§ **Parameters & Config** â€” define date ranges, table names, and selected fields.  \n",
    "2. ğŸ“¥ **Load Raw SigninLogs** â€” read source logs, expand location details, and prepare base DataFrame.  \n",
    "3. ğŸ”„ **Backfill Loop** â€” iterate day by day, compute candidate features + daily stats, save to datalake.  \n",
    "4. ğŸ“Š **Preview Outputs** â€” inspect schemas and sample rows for both output tables.  \n",
    "5. âš ï¸ **Optional Reset** â€” safeguard-protected section to delete tables if a re-run is required.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Data Flow (Mermaid Diagram)\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[ğŸ“¥ SigninLogs<br/>Raw Data] --> B[ğŸ” Preprocessing<br/>Select Fields + Expand JSON]\n",
    "    B --> C[ğŸ§® Candidate Features<br/> Attempts, Users, Entropy, Geo, ASN]\n",
    "    B --> D[ğŸ“Š Daily Stats<br/> Totals, Distinct IPs/Users, Lockouts, Successes]\n",
    "\n",
    "    C --> E[ğŸ’¾ password_spray_features<br/>Table]\n",
    "    D --> F[ğŸ’¾ signin_stats_daily<br/>Table]\n",
    "\n",
    "    E --> G[ğŸ“ˆ Dashboards & Reports]\n",
    "    F --> G\n",
    "    E --> H[ğŸš¨ Alerts]\n",
    "    F --> H\n",
    "    E --> I[ğŸ•µï¸ Investigations]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6ceac",
   "metadata": {},
   "source": [
    "# ğŸ”§ Parameters & Config\n",
    "\n",
    "This section defines the **runtime parameters** and ensures the notebook can be easily reused or adapted.\n",
    "\n",
    "- ğŸ“… **Date ranges** â€” specify the `backfill_start_date` and `backfill_end_date` along with the rolling `lookback_window_days`.  \n",
    "- ğŸ—‚ï¸ **Table names** â€” input (`SigninLogs`) and output (`password_spray_features`, `signin_stats_daily`) tables are set here for easy swapping.  \n",
    "- ğŸ“ **Selected fields** â€” only the most relevant fields are read (User, IP, ASN, Location, Status, etc.), reducing processing overhead.  \n",
    "- âš™ï¸ **Write options** â€” define how results are written (append mode preserves history).  \n",
    "\n",
    "This modular setup means analysts can quickly adjust **dates, fields, or destinations** without touching the core logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8825922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Lookback range (e.g., last 30 days up to yesterday)\n",
    "lookback_days = 4\n",
    "backfill_end_date   = datetime.now().date() - timedelta(days=1)   # yesterday\n",
    "backfill_start_date = backfill_end_date - timedelta(days=lookback_days-1)\n",
    "\n",
    "# Workspace name (update for your environment)\n",
    "workspace_name = \"Woodgrove-LogAnalyiticsWorkspace\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table = \"SigninLogs\"\n",
    "output_datalake_summary_table = \"signin_summary_daily_SPRK\"\n",
    "output_datalake_stats_table   = \"signin_stats_daily_SPRK\"\n",
    "output_datalake_features_table = \"password_spray_features_SPRK\"\n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserDisplayName\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment -natively available in SigninLogs\n",
    "    \"LocationDetails\",         # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\"\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"ğŸ“… Backfill Parameters\")\n",
    "print(f\"   Start â†’ End: {backfill_start_date} â†’ {backfill_end_date}\")\n",
    "print(f\"   Lookback:   {lookback_days} days\\n\")\n",
    "\n",
    "print(\"ğŸ“‚ Tables\")\n",
    "print(f\"   Input:      {input_table}\")\n",
    "print(f\"   Signins Summary: {output_datalake_summary_table}\")\n",
    "print(f\"   Signins Stats: {output_datalake_stats_table}\")\n",
    "print(f\"   Password Spray Features:      {output_datalake_features_table}\\n\")\n",
    "\n",
    "print(\"ğŸ“ Selected Fields:\")\n",
    "print(\"   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62f7a6",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Azure AD `SigninLogs` table.  \n",
    "\n",
    "- ğŸ“Œ **Purpose**: Focus only on fields relevant to password spray detection.  \n",
    "- ğŸŒ **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).  \n",
    "- ğŸ”¢ **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.  \n",
    "- ğŸ“… **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.  \n",
    "\n",
    "âœ… At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109b56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_df = (\n",
    "    data_provider.read_table(input_table, workspace_name)\n",
    "        .select(*signin_fields)\n",
    "        .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "        .filter((F.col(\"TimeGenerated\") >= F.lit(backfill_start_date)) &\n",
    "                (F.col(\"TimeGenerated\") <  F.lit(backfill_end_date + timedelta(days=1))))\n",
    "        # Expand LocationDetails JSON\n",
    "        .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "        .withColumn(\"State\", F.get_json_object(\"LocationDetails\", \"$.state\"))\n",
    "        .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "        .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "        .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "        # Rename ASN for consistency\n",
    "        .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    ")\n",
    "\n",
    "signin_df.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"âœ… Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9858ab",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ”„ Backfill Loop\n",
    "\n",
    "The heart of the notebook â€” iterating from `backfill_start_date` â†’ `backfill_end_date`.  \n",
    "For each day, we build **two sets of outputs**: *features* and *daily stats*.\n",
    "\n",
    "### Step-by-Step\n",
    "1. ğŸ” **Select rolling window**  \n",
    "   - Extract a sliding window of activity (`day - lookback_window_days` â†’ `day`).  \n",
    "   - Ensures we capture **low-and-slow spray patterns** that span multiple days.  \n",
    "\n",
    "2. ğŸ§® **Compute candidate features**  \n",
    "   - Aggregate per IP: total attempts, distinct targeted users, days active.  \n",
    "   - Compute entropy of usernames (distribution spread).  \n",
    "   - Normalize metrics and calculate a **spray_score**.  \n",
    "   - Add **spray_score_label** (LOW / MEDIUM / HIGH).  \n",
    "   - Include attribution fields (IP, ASN, City, Country).  \n",
    "\n",
    "3. ğŸ’¾ **Append to `password_spray_features`**  \n",
    "   - Stores per-IP features with scores and context.  \n",
    "   - Enables downstream dashboards, detections, and investigations.  \n",
    "\n",
    "4. ğŸ“Š **Compute daily stats**  \n",
    "   - Aggregate per day: total attempts, distinct targeted users, distinct source IPs.  \n",
    "   - Count lockouts (ResultType=50053) and successes.  \n",
    "\n",
    "5. ğŸ’¾ **Append to `signins_stats_daily`**  \n",
    "   - Provides daily rollups to track attack volume and trends.  \n",
    "   - Useful for high-level reporting and long-term baselines.  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“Œ Together, these outputs provide **both granular IP-level signals** (features) and **strategic daily summaries** (stats), enabling analysts to detect and contextualize password spray activity.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph BackfillLoop[Backfill Loop - Daily]\n",
    "        A[Current day = start_date] --> B[Define Lookback Window<br/>day - N days â†’ day]\n",
    "        B --> C[Filter SigninLogs for Window]\n",
    "        C --> D[Compute Candidate Features<br/>per IP]\n",
    "        D --> E[Write to password_spray_features<br/>partition run_date = day]\n",
    "        B --> F[Filter SigninLogs for Current Day Only]\n",
    "        F --> G[Aggregate Daily Stats]\n",
    "        G --> H[Write to signins_stats_daily<br/>row for date = day]\n",
    "        E --> I[Increment Day â†’ Next]\n",
    "        H --> I\n",
    "        I -->|Loop until end_date| A\n",
    "    end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c1be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backfill Loop\n",
    "# -----------------------------\n",
    "print(f\" Backfilling the data range: {backfill_start_date} â†’ {backfill_end_date}\")\n",
    "current = backfill_start_date\n",
    "while current <= backfill_end_date:\n",
    "    try:\n",
    "        # Define day window (midnight â†’ next midnight UTC)\n",
    "        run_start = datetime(current.year, current.month, current.day)  # midnight start\n",
    "        run_end   = run_start + timedelta(days=1)                       # midnight next day\n",
    "\n",
    "        print(f\"â–¶ï¸ Processing {current} â†’ {run_start} to {run_end}\")\n",
    "\n",
    "        print(f\"â–¶ï¸ Currently Processing {current}...\")\n",
    "\n",
    "        # Load raw logs for this day\n",
    "        df_day = (\n",
    "            data_provider.read_table(input_table, workspace_name)\n",
    "                .select(*signin_fields)\n",
    "                .filter((F.col(\"TimeGenerated\") >= F.lit(run_start)) &\n",
    "                        (F.col(\"TimeGenerated\") <  F.lit(run_end)))\n",
    "                .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "                .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "                .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "                .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "                .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "                .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    "                .cache()\n",
    "        )\n",
    "\n",
    "        # ------------------\n",
    "        # Daily summary per IP\n",
    "        # ------------------\n",
    "        agg_all = (\n",
    "            df_day.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\", \"date\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"attempts_total\"),\n",
    "                    F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "                    F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "                    F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "                    F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        user_counts = df_day.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "        entropy = (\n",
    "            user_counts\n",
    "                .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "                .groupBy(\"IPAddress\")\n",
    "                .agg(\n",
    "                    F.round(-F.sum(F.col(\"p\") * F.log2(\"p\")), 2).alias(\"username_entropy\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        summary = agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "\n",
    "        data_provider.save_as_table(\n",
    "            summary,\n",
    "            output_datalake_summary_table,\n",
    "            write_options={\"mode\": \"append\", \"partitionBy\": \"date\"}\n",
    "        )\n",
    "        print(f\"âœ… Wrote summary for {current}\")\n",
    "\n",
    "        # ------------------\n",
    "        # Daily stats rollup\n",
    "        # ------------------\n",
    "        stats = (\n",
    "            df_day.groupBy(\"date\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"total_attempts\"),\n",
    "                    F.countDistinct(\"UserPrincipalName\").alias(\"distinct_targeted_users\"),\n",
    "                    F.countDistinct(\"IPAddress\").alias(\"distinct_source_ips\"),\n",
    "                    F.sum(F.when(F.col(\"ResultType\")==\"50053\",1).otherwise(0)).alias(\"lockouts\"),\n",
    "                    F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"successes\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        # data_provider.save_as_table(\n",
    "        #     stats,\n",
    "        #     output_datalake_stats_table,\n",
    "        #     write_options={\"mode\": \"append\", \"partitionBy\": \"date\"}\n",
    "        # )\n",
    "        print(f\"âœ… Wrote stats for {current}\")\n",
    "\n",
    "        # Clean up\n",
    "        df_day.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {current}: {e}\")\n",
    "\n",
    "    # Move to next day\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(\"ğŸ‰ Backfill complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backfill Loop\n",
    "# -----------------------------\n",
    "print(f\" Backfilling the data range: {backfill_start_date} â†’ {backfill_end_date}\")\n",
    "current = backfill_start_date\n",
    "while current <= backfill_end_date:\n",
    "    try:\n",
    "        window_start = current - timedelta(days=3)\n",
    "        window_end   = current\n",
    "\n",
    "        # -----------------\n",
    "        # Filter once & cache\n",
    "        # -----------------\n",
    "        df_window = (\n",
    "            signin_df.filter(\n",
    "                (F.col(\"date\") >= F.lit(window_start.isoformat())) &\n",
    "                (F.col(\"date\") <= F.lit(window_end.isoformat()))\n",
    "            )\n",
    "            .cache()\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Candidate features (single groupBy)\n",
    "        # Carry over ASN, City, Country for attribution\n",
    "        # -----------------\n",
    "        agg_all = (\n",
    "            df_window.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\")\n",
    "              .agg(\n",
    "                F.count(\"*\").alias(\"attempts_total\"),\n",
    "                F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "                F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "                F.countDistinct(\"date\").alias(\"days_active\"),\n",
    "                F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "                F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "              )\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Entropy (optimized with window instead of join)\n",
    "        # -----------------\n",
    "        user_counts = df_window.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "        entropy = (\n",
    "            user_counts\n",
    "              .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "              .groupBy(\"IPAddress\")\n",
    "              .agg((-F.sum(F.col(\"p\")*F.log2(\"p\"))).alias(\"username_entropy\"))\n",
    "        )\n",
    "\n",
    "        features = (\n",
    "            agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "                   .withColumn(\"run_date\", F.lit(current.isoformat()))\n",
    "                   .withColumn(\"detection_window_start\", F.lit(window_start.isoformat()))\n",
    "                   .withColumn(\"detection_window_end\", F.lit(window_end.isoformat()))\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Normalization (reuse global precomputed values if available)\n",
    "        # -----------------\n",
    "        if 'max_users' not in globals() or 'max_entropy' not in globals():\n",
    "            # compute once, outside loop ideally\n",
    "            global_max = features.agg(\n",
    "                F.max(\"distinct_users\").alias(\"max_users\"),\n",
    "                F.round(F.max(\"username_entropy\"), 2).alias(\"max_entropy\")\n",
    "            ).first()\n",
    "\n",
    "            max_users = global_max.max_users or 1\n",
    "            max_entropy = global_max.max_entropy or 1.0\n",
    "\n",
    "        features = (\n",
    "            features\n",
    "            .withColumn(\"distinct_users_norm\", F.round(F.col(\"distinct_users\") / F.lit(max_users), 2))\n",
    "            .withColumn(\"success_rate\", F.round(F.col(\"success_count\") / F.greatest(F.col(\"attempts_total\"), F.lit(1)), 2))\n",
    "            .withColumn(\"entropy_norm\", F.round(F.col(\"username_entropy\") / F.lit(max_entropy), 2))\n",
    "            .withColumn(\n",
    "                \"spray_score\",\n",
    "                F.round(\n",
    "                    0.5*F.col(\"distinct_users_norm\") +\n",
    "                    0.2*(1 - F.col(\"success_rate\")) +\n",
    "                    0.3*F.col(\"entropy_norm\"),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"spray_score_label\",\n",
    "                F.when(F.col(\"spray_score\") < 0.3, F.lit(\"LOW\"))\n",
    "                .when(F.col(\"spray_score\") < 0.6, F.lit(\"MEDIUM\"))\n",
    "                .otherwise(F.lit(\"HIGH\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Write candidates\n",
    "        data_provider.save_as_table(features, output_datalake_features_table, write_options=write_options)\n",
    "        print(f\"âœ… Wrote candidates for {current}\")\n",
    "\n",
    "        # Unpersist cached slice\n",
    "        df_window.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {current}: {str(e)}\")\n",
    "\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(\"ğŸ‰ Backfill complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26220e44",
   "metadata": {},
   "source": [
    "# ğŸ‘€ Preview Outputs\n",
    "\n",
    "After running the backfill loop, itâ€™s important to validate that the pipeline produced the expected data.  \n",
    "This section shows **schemas** and **sample rows** for both output tables.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ—‚ï¸ Daily Summary Table (`signin_summary_daily_SPRK`)\n",
    "\n",
    "This table provides **per-IP, per-day aggregates** of sign-in activity.  \n",
    "It compresses raw `SigninLogs` into a daily rollup for each source IP, while preserving attribution context.  \n",
    "\n",
    "- ğŸŒ **Geo & ASN context** â†’ IPAddress, ASN, City, Country  \n",
    "- ğŸ“… **Date** â†’ reporting day of the aggregation  \n",
    "- ğŸ”¢ **Total attempts** â†’ total number of authentication attempts from the IP on that day  \n",
    "- âœ… **Success count** â†’ number of successful logons (helps measure spray effectiveness)  \n",
    "- ğŸ‘¤ **Distinct users** â†’ number of unique targeted accounts  \n",
    "- â±ï¸ **First seen / Last seen** â†’ earliest and latest attempt timestamps for that IP within the day  \n",
    "- ğŸ§® **Username entropy** â†’ entropy score measuring spread/randomness of targeted usernames  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Daily Stats Table (`signin_stats_daily_SPRK`)\n",
    "\n",
    "This table provides **daily rollups** of authentication activity.  \n",
    "It helps track the overall level of spray attempts and anomalies over time.\n",
    "\n",
    "- ğŸ“… **Date** â†’ reporting date  \n",
    "- ğŸ”¢ **Total attempts** â†’ all sign-in attempts for the day  \n",
    "- ğŸ‘¤ **Distinct targeted users** â†’ how many unique accounts were hit  \n",
    "- ğŸŒ **Distinct source IPs** â†’ how many unique IPs attempted logons  \n",
    "- ğŸš« **Lockouts** â†’ counts of account lockout events (ResultType=50053)  \n",
    "- âœ… **Successes** â†’ counts of successful logons  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© Features Table (`password_spray_features_SPRK`)\n",
    "\n",
    "This table contains **per-IP aggregated features** across the rolling lookback window.  \n",
    "It helps identify which IP addresses exhibit password spray-like behavior.\n",
    "\n",
    "- ğŸŒ **Geo & ASN context** â†’ IPAddress, ASN, City, Country  \n",
    "- ğŸ“Š **Behavioral metrics** â†’ attempts_total, distinct_users, days_active, entropy  \n",
    "- ğŸ”¢ **Normalized features** â†’ distinct_users_norm, entropy_norm, success_rate  \n",
    "- ğŸ¯ **Spray score** â†’ weighted score reflecting spray likelihood  \n",
    "- ğŸ·ï¸ **Spray score label** â†’ LOW / MEDIUM / HIGH for easier triage  \n",
    "\n",
    "---\n",
    "\n",
    "These outputs form the foundation for:\n",
    "- ğŸ“ˆ **Dashboards** (geo heatmaps, trend lines, score distributions)  \n",
    "- ğŸš¨ **Alerts** (triggering on high-score IPs)  \n",
    "- ğŸ•µï¸ **Investigations** (pivoting into ASN, city, or recurring IPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f54b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Preview sample outputs\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nğŸ“‘ Daily Summary Table Schema\")\n",
    "summary.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Daily Summary Sample Rows\")\n",
    "display(\n",
    "    summary.select(\n",
    "        \"date\", \"IPAddress\", \"attempts_total\", \"success_count\",\n",
    "        \"distinct_users\", \"username_entropy\", \"ASN\", \"City\", \"Country\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‘ Daily Stats Table Schema\")\n",
    "stats.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Daily Stats Sample Rows\")\n",
    "display(\n",
    "    stats.select(\n",
    "        \"date\", \"total_attempts\", \"distinct_targeted_users\",\n",
    "        \"distinct_source_ips\", \"lockouts\", \"successes\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‘ Candidates Table Schema\")\n",
    "features.printSchema()   # Prints schema only (no scan)\n",
    "\n",
    "print(\"\\nğŸ” Candidates Sample Rows\")\n",
    "display(\n",
    "    features.select(\n",
    "        \"IPAddress\", \"ASN\", \"City\", \"Country\",\n",
    "        \"attempts_total\", \"distinct_users\", \"days_active\",\n",
    "        \"username_entropy\", \"spray_score\", \"spray_score_label\"\n",
    "    ).limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930087a5",
   "metadata": {},
   "source": [
    "# âš ï¸ Delete Entire Datalake Tables\n",
    "### Proceed with Caution\n",
    "\n",
    "Occasionally you may need to **reset** the output tables if the backfill wrote incorrect data.  \n",
    "Use this section **with extreme caution**:\n",
    "\n",
    "- ğŸš¨ This will **permanently delete** the table from the datalake.  \n",
    "- ğŸ”’ Keep the command **commented out by default**.  \n",
    "- âœ… Only **uncomment** if you really intend to wipe and rebuild the table.  \n",
    "\n",
    "Recommended workflow:  \n",
    "1. Double-check table names (`features` vs `stats`) before deletion.  \n",
    "2. Run deletion only in a controlled/test environment.  \n",
    "3. Immediately rerun the notebook to regenerate fresh data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06c457be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Uncomment the relevant block AND set confirm_delete = True to enable deletion\n",
    "\n",
    "confirm_delete = False   # Change to True only if you want to proceed\n",
    "\n",
    "if confirm_delete:\n",
    "    # Delete candidate table\n",
    "    # data_provider.delete_table(output_datalake_summary_table)\n",
    "    \n",
    "    # Delete daily stats table\n",
    "    # data_provider.delete_table(output_datalake_stats_table)\n",
    "\n",
    "    # Delete password spray features table\n",
    "    # data_provider.delete_table(output_datalake_features_table)\n",
    "    \n",
    "    print(\"âš ï¸ Tables deleted. You must rerun the backfill to regenerate data.\")\n",
    "else:\n",
    "    print(\"âœ… Delete set to False, Safeguard active. No tables were deleted or action needed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [00_data_backfill_setup]",
   "language": "Python",
   "name": "MSGMedium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
