{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b90d51",
   "metadata": {},
   "source": [
    "# 🔐 Password Spray Backfill Notebook\n",
    "\n",
    "## 📖 Overview\n",
    "\n",
    "This notebook is designed to **backfill historical data** from Microsoft Entra ID `SigninLogs` into two derived tables:\n",
    "\n",
    "- 🧩 **Features Table** → Per-IP behavioral features with spray score and labels.\n",
    "- 📊 **Daily Stats Table** → Rollup metrics of sign-in activity per day.\n",
    "\n",
    "It provides both **long lookback detection** (to uncover low-and-slow password spray campaigns) and **daily snapshots** for monitoring trends.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Objectives\n",
    "\n",
    "- ✅ Load raw `SigninLogs` with relevant fields (user, IP, ASN, geo, status).\n",
    "- ✅ Expand JSON location details into structured columns (City, Country, Latitude, Longitude).\n",
    "- ✅ Compute **rolling window features** per IP: attempts, distinct users, days active, entropy.\n",
    "- ✅ Calculate a **spray score** and assign **labels** (LOW / MEDIUM / HIGH).\n",
    "- ✅ Write results to dedicated Sentinel data lake tables for further use (detections, dashboards, investigations).\n",
    "- ✅ Provide preview of schema and sample rows for validation.\n",
    "- ✅ Include safeguards for table deletion in case of incorrect backfill.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Workflow\n",
    "\n",
    "1. 🔧 **Parameters & Config** — define date ranges, table names, and selected fields.\n",
    "2. 📥 **Load Raw SigninLogs** — read source logs, expand location details, and prepare base DataFrame.\n",
    "3. 🔄 **Backfill Loop** — iterate day by day, compute candidate features + daily stats, save to Sentinel data lake.\n",
    "4. 📊 **Preview Outputs** — inspect schemas and sample rows for both output tables.\n",
    "5. ⚠️ **Optional Reset** — safeguard-protected section to delete tables if a re-run is required.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗺️ Data Flow (Mermaid Diagram)\n",
    "\n",
    "````mermaid\n",
    "flowchart TD\n",
    "    A[📥 SigninLogs<br/>Raw Data] --> B[🔍 Preprocessing<br/>Select Fields + Expand JSON]\n",
    "    B --> C[🧮 Candidate Features<br/> Attempts, Users, Entropy, Geo, ASN]\n",
    "    B --> D[📊 Daily Stats<br/> Totals, Distinct IPs/Users, Lockouts, Successes]\n",
    "\n",
    "    C --> E[💾 password_spray_features<br/>Table]\n",
    "    D --> F[💾 signin_stats_daily_SPRK<br/>Table]\n",
    "\n",
    "    E --> G[📈 Dashboards & Reports]\n",
    "    F --> G\n",
    "    E --> H[🚨 Alerts]\n",
    "    F --> H\n",
    "    E --> I[🕵️ Investigations]\n",
    "    ```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6ceac",
   "metadata": {},
   "source": [
    "# 🔧 Parameters & Config\n",
    "\n",
    "This section defines the **runtime parameters** and ensures the notebook can be easily reused or adapted.\n",
    "\n",
    "- 📅 **Date ranges** — specify the `backfill_start_date` and `backfill_end_date` along with the rolling `lookback_window_days`.\n",
    "- 🗂️ **Table names** — input (`SigninLogs`) and output (`password_spray_features_SPRK`, `signin_stats_daily_SPRK`) tables are set here for easy swapping.\n",
    "- 📝 **Selected fields** — only the most relevant fields are read (User, IP, ASN, Location, Status, etc.), reducing processing overhead.\n",
    "- ⚙️ **Write options** — define how results are written (append mode preserves history).\n",
    "\n",
    "This modular setup means analysts can quickly adjust **dates, fields, or destinations** without touching the core logic.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8825922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Lookback range (e.g., last 30 days up to yesterday)\n",
    "lookback_days = 4\n",
    "backfill_end_date = datetime.now().date() - timedelta(days=1)  # yesterday\n",
    "backfill_start_date = backfill_end_date - timedelta(days=lookback_days - 1)\n",
    "\n",
    "# Sentinel Workspace name (update for your environment)\n",
    "workspace_name = \"<YourWorkspaceName>\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table = \"SigninLogs\"\n",
    "output_datalake_summary_table = \"signin_summary_daily_SPRK\"\n",
    "output_datalake_stats_table = \"signin_stats_daily_SPRK\"\n",
    "output_datalake_features_table = \"password_spray_features_SPRK\"\n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserDisplayName\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment -natively available in SigninLogs\n",
    "    \"LocationDetails\",  # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"📅 Backfill Parameters\")\n",
    "print(f\"   Start → End: {backfill_start_date} → {backfill_end_date}\")\n",
    "print(f\"   Lookback:   {lookback_days} days\\n\")\n",
    "\n",
    "print(\"📂 Tables\")\n",
    "print(f\"   Input:      {input_table}\")\n",
    "print(f\"   Signins Summary: {output_datalake_summary_table}\")\n",
    "print(f\"   Signins Stats: {output_datalake_stats_table}\")\n",
    "print(f\"   Password Spray Features:      {output_datalake_features_table}\\n\")\n",
    "\n",
    "print(\"📝 Selected Fields:\")\n",
    "print(\"   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62f7a6",
   "metadata": {},
   "source": [
    "## 📥 Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Microsoft Entra AD `SigninLogs` table.\n",
    "\n",
    "- 📌 **Purpose**: Focus only on fields relevant to password spray detection.\n",
    "- 🌍 **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).\n",
    "- 🔢 **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.\n",
    "- 📅 **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.\n",
    "\n",
    "✅ At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109b56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signin_df = (\n",
    "    data_provider.read_table(input_table, workspace_name)\n",
    "        .select(*signin_fields)\n",
    "        .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "        .filter((F.col(\"TimeGenerated\") >= F.lit(backfill_start_date)) &\n",
    "                (F.col(\"TimeGenerated\") <  F.lit(backfill_end_date + timedelta(days=1))))\n",
    "        # Expand LocationDetails JSON\n",
    "        .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "        .withColumn(\"State\", F.get_json_object(\"LocationDetails\", \"$.state\"))\n",
    "        .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "        .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "        .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "        # Rename ASN for consistency\n",
    "        .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    ")\n",
    "\n",
    "signin_df.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"✅ Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9858ab",
   "metadata": {},
   "source": [
    "## 🔄 Backfill Loop\n",
    "\n",
    "The heart of the notebook — iterating from `backfill_start_date` → `backfill_end_date`.  \n",
    "For each day, we build **two sets of outputs**: _features_ and _daily stats_.\n",
    "\n",
    "### Step-by-Step\n",
    "\n",
    "1. 🔍 **Select rolling window**\n",
    "\n",
    "   - Extract a sliding window of activity (`day - lookback_window_days` → `day`).\n",
    "   - Ensures we capture **low-and-slow spray patterns** that span multiple days.\n",
    "\n",
    "2. 🧮 **Compute candidate features**\n",
    "\n",
    "   - Aggregate per IP: total attempts, distinct targeted users, days active.\n",
    "   - Compute entropy of usernames (distribution spread).\n",
    "   - Normalize metrics and calculate a **spray_score**.\n",
    "   - Add **spray_score_label** (LOW / MEDIUM / HIGH).\n",
    "   - Include attribution fields (IP, ASN, City, Country).\n",
    "\n",
    "3. 💾 **Append to `password_spray_features_SPRK`**\n",
    "\n",
    "   - Stores per-IP features with scores and context.\n",
    "   - Enables downstream dashboards, detections, and investigations.\n",
    "\n",
    "4. 📊 **Compute daily stats**\n",
    "\n",
    "   - Aggregate per day: total attempts, distinct targeted users, distinct source IPs.\n",
    "   - Count lockouts (ResultType=50053) and successes.\n",
    "\n",
    "5. 💾 **Append to `signins_stats_daily_SPRK`**\n",
    "   - Provides daily rollups to track attack volume and trends.\n",
    "   - Useful for high-level reporting and long-term baselines.\n",
    "\n",
    "---\n",
    "\n",
    "📌 Together, these outputs provide **both granular IP-level signals** (features) and **strategic daily summaries** (stats), enabling analysts to detect and contextualize password spray activity.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph BackfillLoop[Backfill Loop - Daily]\n",
    "        A[Current day = start_date] --> B[Define Lookback Window<br/>day - N days → day]\n",
    "        B --> C[Filter SigninLogs for Window]\n",
    "        C --> D[Compute Candidate Features<br/>per IP]\n",
    "        D --> E[Write to password_spray_features_SPRK<br/>partition run_date = day]\n",
    "        B --> F[Filter SigninLogs for Current Day Only]\n",
    "        F --> G[Aggregate Daily Stats]\n",
    "        G --> H[Write to signins_stats_daily_SPRK<br/>row for date = day]\n",
    "        E --> I[Increment Day → Next]\n",
    "        H --> I\n",
    "        I -->|Loop until end_date| A\n",
    "    end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c1be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backfill Loop\n",
    "# -----------------------------\n",
    "print(f\" Backfilling the data range: {backfill_start_date} → {backfill_end_date}\")\n",
    "current = backfill_start_date\n",
    "while current <= backfill_end_date:\n",
    "    try:\n",
    "        # Define day window (midnight → next midnight UTC)\n",
    "        run_start = datetime(current.year, current.month, current.day)  # midnight start\n",
    "        run_end   = run_start + timedelta(days=1)                       # midnight next day\n",
    "\n",
    "        print(f\"▶️ Processing {current} → {run_start} to {run_end}\")\n",
    "\n",
    "        print(f\"▶️ Currently Processing {current}...\")\n",
    "\n",
    "        # Load raw logs for this day\n",
    "        df_day = (\n",
    "            data_provider.read_table(input_table, workspace_name)\n",
    "                .select(*signin_fields)\n",
    "                .filter((F.col(\"TimeGenerated\") >= F.lit(run_start)) &\n",
    "                        (F.col(\"TimeGenerated\") <  F.lit(run_end)))\n",
    "                .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "                .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "                .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "                .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "                .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "                .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    "                .cache()\n",
    "        )\n",
    "\n",
    "        # ------------------\n",
    "        # Daily summary per IP\n",
    "        # ------------------\n",
    "        agg_all = (\n",
    "            df_day.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\", \"date\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"attempts_total\"),\n",
    "                    F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "                    F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "                    F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "                    F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        user_counts = df_day.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "        entropy = (\n",
    "            user_counts\n",
    "                .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "                .groupBy(\"IPAddress\")\n",
    "                .agg(\n",
    "                    F.round(-F.sum(F.col(\"p\") * F.log2(\"p\")), 2).alias(\"username_entropy\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        summary = agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "\n",
    "        data_provider.save_as_table(\n",
    "            summary,\n",
    "            output_datalake_summary_table,\n",
    "            write_options={\"mode\": \"append\", \"partitionBy\": \"date\"}\n",
    "        )\n",
    "        print(f\"✅ Wrote summary for {current}\")\n",
    "\n",
    "        # ------------------\n",
    "        # Daily stats rollup\n",
    "        # ------------------\n",
    "        stats = (\n",
    "            df_day.groupBy(\"date\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"total_attempts\"),\n",
    "                    F.countDistinct(\"UserPrincipalName\").alias(\"distinct_targeted_users\"),\n",
    "                    F.countDistinct(\"IPAddress\").alias(\"distinct_source_ips\"),\n",
    "                    F.sum(F.when(F.col(\"ResultType\")==\"50053\",1).otherwise(0)).alias(\"lockouts\"),\n",
    "                    F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"successes\")\n",
    "                )\n",
    "        )\n",
    "\n",
    "        data_provider.save_as_table(\n",
    "            stats,\n",
    "            output_datalake_stats_table,\n",
    "            write_options={\"mode\": \"append\", \"partitionBy\": \"date\"}\n",
    "        )\n",
    "        print(f\"✅ Wrote stats for {current}\")\n",
    "\n",
    "        # Clean up\n",
    "        df_day.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {current}: {e}\")\n",
    "\n",
    "    # Move to next day\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(\"🎉 Backfill complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Backfill Loop\n",
    "# -----------------------------\n",
    "print(f\" Backfilling the data range: {backfill_start_date} → {backfill_end_date}\")\n",
    "current = backfill_start_date\n",
    "while current <= backfill_end_date:\n",
    "    try:\n",
    "        window_start = current - timedelta(days=3)\n",
    "        window_end   = current\n",
    "\n",
    "        # -----------------\n",
    "        # Filter once & cache\n",
    "        # -----------------\n",
    "        df_window = (\n",
    "            signin_df.filter(\n",
    "                (F.col(\"date\") >= F.lit(window_start.isoformat())) &\n",
    "                (F.col(\"date\") <= F.lit(window_end.isoformat()))\n",
    "            )\n",
    "            .cache()\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Candidate features (single groupBy)\n",
    "        # Carry over ASN, City, Country for attribution\n",
    "        # -----------------\n",
    "        agg_all = (\n",
    "            df_window.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\")\n",
    "              .agg(\n",
    "                F.count(\"*\").alias(\"attempts_total\"),\n",
    "                F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "                F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "                F.countDistinct(\"date\").alias(\"days_active\"),\n",
    "                F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "                F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "              )\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Entropy (optimized with window instead of join)\n",
    "        # -----------------\n",
    "        user_counts = df_window.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "        entropy = (\n",
    "            user_counts\n",
    "              .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "              .groupBy(\"IPAddress\")\n",
    "              .agg((-F.sum(F.col(\"p\")*F.log2(\"p\"))).alias(\"username_entropy\"))\n",
    "        )\n",
    "\n",
    "        features = (\n",
    "            agg_all.join(entropy, \"IPAddress\", \"left\")\n",
    "                   .withColumn(\"run_date\", F.lit(current.isoformat()))\n",
    "                   .withColumn(\"detection_window_start\", F.lit(window_start.isoformat()))\n",
    "                   .withColumn(\"detection_window_end\", F.lit(window_end.isoformat()))\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Normalization (reuse global precomputed values if available)\n",
    "        # -----------------\n",
    "        if 'max_users' not in globals() or 'max_entropy' not in globals():\n",
    "            # compute once, outside loop ideally\n",
    "            global_max = features.agg(\n",
    "                F.max(\"distinct_users\").alias(\"max_users\"),\n",
    "                F.round(F.max(\"username_entropy\"), 2).alias(\"max_entropy\")\n",
    "            ).first()\n",
    "\n",
    "            max_users = global_max.max_users or 1\n",
    "            max_entropy = global_max.max_entropy or 1.0\n",
    "\n",
    "        features = (\n",
    "            features\n",
    "            .withColumn(\"distinct_users_norm\", F.round(F.col(\"distinct_users\") / F.lit(max_users), 2))\n",
    "            .withColumn(\"success_rate\", F.round(F.col(\"success_count\") / F.greatest(F.col(\"attempts_total\"), F.lit(1)), 2))\n",
    "            .withColumn(\"entropy_norm\", F.round(F.col(\"username_entropy\") / F.lit(max_entropy), 2))\n",
    "            .withColumn(\n",
    "                \"spray_score\",\n",
    "                F.round(\n",
    "                    0.5*F.col(\"distinct_users_norm\") +\n",
    "                    0.2*(1 - F.col(\"success_rate\")) +\n",
    "                    0.3*F.col(\"entropy_norm\"),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"spray_score_label\",\n",
    "                F.when(F.col(\"spray_score\") < 0.3, F.lit(\"LOW\"))\n",
    "                .when(F.col(\"spray_score\") < 0.6, F.lit(\"MEDIUM\"))\n",
    "                .otherwise(F.lit(\"HIGH\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Write candidates\n",
    "        data_provider.save_as_table(features, output_datalake_features_table, write_options=write_options)\n",
    "        print(f\"✅ Wrote candidates for {current}\")\n",
    "\n",
    "        # Unpersist cached slice\n",
    "        df_window.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {current}: {str(e)}\")\n",
    "\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(\"🎉 Backfill complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26220e44",
   "metadata": {},
   "source": [
    "# 👀 Preview Outputs\n",
    "\n",
    "After running the backfill loop, it’s important to validate that the pipeline produced the expected data.  \n",
    "This section shows **schemas** and **sample rows** for both output tables.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂️ Daily Summary Table (`signin_summary_daily_SPRK`)\n",
    "\n",
    "This table provides **per-IP, per-day aggregates** of sign-in activity.  \n",
    "It compresses raw `SigninLogs` into a daily rollup for each source IP, while preserving attribution context.\n",
    "\n",
    "- 🌍 **Geo & ASN context** → IPAddress, ASN, City, Country\n",
    "- 📅 **Date** → reporting day of the aggregation\n",
    "- 🔢 **Total attempts** → total number of authentication attempts from the IP on that day\n",
    "- ✅ **Success count** → number of successful logons (helps measure spray effectiveness)\n",
    "- 👤 **Distinct users** → number of unique targeted accounts\n",
    "- ⏱️ **First seen / Last seen** → earliest and latest attempt timestamps for that IP within the day\n",
    "- 🧮 **Username entropy** → entropy score measuring spread/randomness of targeted usernames\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Daily Stats Table (`signin_stats_daily_SPRK`)\n",
    "\n",
    "This table provides **daily rollups** of authentication activity.  \n",
    "It helps track the overall level of spray attempts and anomalies over time.\n",
    "\n",
    "- 📅 **Date** → reporting date\n",
    "- 🔢 **Total attempts** → all sign-in attempts for the day\n",
    "- 👤 **Distinct targeted users** → how many unique accounts were hit\n",
    "- 🌐 **Distinct source IPs** → how many unique IPs attempted logons\n",
    "- 🚫 **Lockouts** → counts of account lockout events (ResultType=50053)\n",
    "- ✅ **Successes** → counts of successful logons\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Features Table (`password_spray_features_SPRK`)\n",
    "\n",
    "This table contains **per-IP aggregated features** across the rolling lookback window.  \n",
    "It helps identify which IP addresses exhibit password spray-like behavior.\n",
    "\n",
    "- 🌍 **Geo & ASN context** → IPAddress, ASN, City, Country\n",
    "- 📊 **Behavioral metrics** → attempts_total, distinct_users, days_active, entropy\n",
    "- 🔢 **Normalized features** → distinct_users_norm, entropy_norm, success_rate\n",
    "- 🎯 **Spray score** → weighted score reflecting spray likelihood\n",
    "- 🏷️ **Spray score label** → LOW / MEDIUM / HIGH for easier triage\n",
    "\n",
    "---\n",
    "\n",
    "These outputs form the foundation for:\n",
    "\n",
    "- 📈 **Dashboards** (geo heatmaps, trend lines, score distributions)\n",
    "- 🚨 **Alerts** (triggering on high-score IPs)\n",
    "- 🕵️ **Investigations** (pivoting into ASN, city, or recurring IPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f54b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Preview sample outputs\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\n📑 Daily Summary Table Schema\")\n",
    "summary.printSchema()\n",
    "\n",
    "print(\"\\n🔍 Daily Summary Sample Rows\")\n",
    "display(\n",
    "    summary.select(\n",
    "        \"date\", \"IPAddress\", \"attempts_total\", \"success_count\",\n",
    "        \"distinct_users\", \"username_entropy\", \"ASN\", \"City\", \"Country\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\n📑 Daily Stats Table Schema\")\n",
    "stats.printSchema()\n",
    "\n",
    "print(\"\\n🔍 Daily Stats Sample Rows\")\n",
    "display(\n",
    "    stats.select(\n",
    "        \"date\", \"total_attempts\", \"distinct_targeted_users\",\n",
    "        \"distinct_source_ips\", \"lockouts\", \"successes\"\n",
    "    ).limit(10)\n",
    ")\n",
    "\n",
    "print(\"📑 Candidates Table Schema\")\n",
    "features.printSchema()   # Prints schema only (no scan)\n",
    "\n",
    "print(\"\\n🔍 Candidates Sample Rows\")\n",
    "display(\n",
    "    features.select(\n",
    "        \"IPAddress\", \"ASN\", \"City\", \"Country\",\n",
    "        \"attempts_total\", \"distinct_users\", \"days_active\",\n",
    "        \"username_entropy\", \"spray_score\", \"spray_score_label\"\n",
    "    ).limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930087a5",
   "metadata": {},
   "source": [
    "# ⚠️ Delete entire Sentinel data lake tables\n",
    "\n",
    "### Proceed with Caution\n",
    "\n",
    "Occasionally you may need to **reset** the output tables if the backfill wrote incorrect data.  \n",
    "Use this section **with extreme caution**:\n",
    "\n",
    "- 🚨 This will **permanently delete** the table from the Sentinel data lake.\n",
    "- 🔒 Keep the command **commented out by default**.\n",
    "- ✅ Only **uncomment** if you really intend to wipe and rebuild the table.\n",
    "\n",
    "Recommended workflow:\n",
    "\n",
    "1. Double-check table names (`features` vs `stats`) before deletion.\n",
    "2. Run deletion only in a controlled/test environment.\n",
    "3. Immediately rerun the notebook to regenerate fresh data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06c457be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ Uncomment the relevant block AND set confirm_delete = True to enable deletion\n",
    "\n",
    "confirm_delete = False   # Change to True only if you want to proceed\n",
    "\n",
    "if confirm_delete:\n",
    "    # Delete candidate table\n",
    "    # data_provider.delete_table(output_datalake_summary_table)\n",
    "    \n",
    "    # Delete daily stats table\n",
    "    # data_provider.delete_table(output_datalake_stats_table)\n",
    "\n",
    "    # Delete password spray features table\n",
    "    # data_provider.delete_table(output_datalake_features_table)\n",
    "    \n",
    "    print(\"⚠️ Tables deleted. You must rerun the backfill to regenerate data.\")\n",
    "else:\n",
    "    print(\"✅ Delete set to False, Safeguard active. No tables were deleted or action needed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [00_data_backfill_setup]",
   "language": "Python",
   "name": "MSGMedium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
