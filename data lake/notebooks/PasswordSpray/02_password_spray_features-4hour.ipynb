{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8647baf6",
   "metadata": {},
   "source": [
    "# 🔐 Password Spray Features Notebook\n",
    "\n",
    "## 📖 Overview\n",
    "This notebook generates the **Password Spray Features Table** by combining recent raw `SigninLogs` with historical rollups from `signin_summary_daily`.  \n",
    "It enriches per-IP behavior with normalized metrics, a composite spray score, and categorical labels for triage.  \n",
    "\n",
    "This design allows efficient detection of:\n",
    "- 🚨 **High-volume sprays** (short bursts visible in raw logs)  \n",
    "- 🐢 **Low-and-slow sprays** (persistent activity captured across 30-day lookbacks via summary table)  \n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Objectives\n",
    "- ✅ Load recent raw `SigninLogs` and expand JSON location details into structured fields (City, Country, ASN).  \n",
    "- ✅ Load historical `signin_summary_daily` for N-day lookback without rescanning raw logs.  \n",
    "- ✅ Compute per-IP aggregated features: attempts, successes, distinct users, days active, entropy.  \n",
    "- ✅ Normalize features and calculate a **spray_score** using weighted components:  \n",
    "  - `distinct_users_norm` → breadth of targeted accounts  \n",
    "  - `success_rate` → effectiveness of attempts  \n",
    "  - `entropy_norm` → randomness/distribution of usernames  \n",
    "- ✅ Assign categorical labels (**LOW / MEDIUM / HIGH**) based on thresholds.  \n",
    "- ✅ Write enriched feature rows into `password_spray_features`, partitioned by run date.  \n",
    "- ✅ Provide schema + sample preview for validation.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Workflow\n",
    "1. 🔧 **Parameters & Config** — define lookback period, table names, selected fields.  \n",
    "2. 📥 **Load Recent Raw Logs** — ingest `SigninLogs` for the last few hours/day, expand location JSON into structured columns.  \n",
    "3. 🧮 **Aggregate Recent Data** — compute per-IP counts (attempts, distinct users, successes, first/last seen) and entropy.  \n",
    "4. 📅 **Load Historical Summary** — pull per-IP daily rollups from `signin_summary_daily` covering the lookback window.  \n",
    "5. 🔗 **Combine History + Recent** — merge both datasets into a unified summary.  \n",
    "6. 📊 **Generate Features** — aggregate across the merged window to produce normalized features.  \n",
    "7. 🎯 **Compute Spray Score & Labels** — apply formula and assign LOW/MEDIUM/HIGH classification.  \n",
    "8. 💾 **Write to Features Table** — save results into `password_spray_features`, partitioned by run_date.  \n",
    "9. 👀 **Preview Outputs** — inspect schema and sample rows for validation.  \n",
    "10. ⚠️ **Optional Reset** — commented safeguard to delete table if re-run is required.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🗺️ Data Flow (Mermaid Diagram)\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% Inputs\n",
    "    A[📥 SigninLogs Raw Data - Recent] --> A1[📑 Select Columns<br/>User, IP, ASN, Location, Result*]\n",
    "    B[💾 signin_summary_daily Historical Rollups] --> B1[📅 Filter Last N Days<br/>lookback window]\n",
    "\n",
    "    %% Recent Processing\n",
    "    A1 --> C[🧮 Aggregate per IP Recent<br/>attempts_total, success_count,<br/>distinct_users, first_seen, last_seen]\n",
    "    A1 --> D[🧮 Username Entropy Recent<br/>calc p=count/sum, -Σ p*log2 p]\n",
    "    C --> E[🔗 Join Aggregates and Entropy]\n",
    "    D --> E\n",
    "\n",
    "    %% Combine with History\n",
    "    B1 --> F[🔗 Union Recent Summary and Historical Summary]\n",
    "    E --> F\n",
    "\n",
    "    %% Feature Aggregates\n",
    "    F --> G[📊 Group per IP<br/>Σ attempts_total, Σ success_count,<br/>Σ distinct_users, count days_active,<br/>avg entropy, first_seen, last_seen]\n",
    "\n",
    "    %% Normalization & Scoring\n",
    "    G --> H[⚖️ Normalize Features<br/>distinct_users_norm, success_rate,<br/>entropy_norm]\n",
    "    H --> I[🎯 Compute Spray Score<br/>0.5*users_norm + 0.2* 1-success_rate<br/>+ 0.3*entropy_norm]\n",
    "    I --> J[🏷️ Assign Label by Score<br/>LOW <0.3, MEDIUM <0.6, HIGH]\n",
    "\n",
    "    %% Output\n",
    "    J --> K[💾 password_spray_features<br/>partitioned by run_date]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a8d0a",
   "metadata": {},
   "source": [
    "# 🔧 Parameters & Config\n",
    "\n",
    "This section defines the **runtime parameters** and ensures the notebook can be reused or adapted easily.  \n",
    "It sets up **time windows, input/output tables, and selected fields** before processing begins.\n",
    "\n",
    "- ⏱️ **Time ranges**  \n",
    "  - `run_start` → start of the current processing window (last 4 hours by default).  \n",
    "  - `run_end` → end of the current processing window.  \n",
    "  - `lookback_days` → number of historical days pulled from `signin_summary_daily` to combine with fresh data.  \n",
    "\n",
    "- 🗂️ **Table names**  \n",
    "  - Raw input → `SigninLogs` (fresh, high-fidelity events).  \n",
    "  - Historical summary → `signin_summary_daily` (compact daily rollups).  \n",
    "  - Output → `password_spray_features` (normalized features, spray score, and labels).  \n",
    "\n",
    "- 📝 **Selected fields**  \n",
    "  - Core identity fields → `UserPrincipalName`, `UserDisplayName`, `UserType`.  \n",
    "  - Request context → `IPAddress`, `ResultType`, `ResultSignature`, `Status`, `UserAgent`.  \n",
    "  - Enrichment fields → `AutonomousSystemNumber`, `LocationDetails` (JSON expanded into City, Country, Latitude, Longitude).  \n",
    "\n",
    "- ⚙️ **Write options**  \n",
    "  - Append mode is used, so historical runs are preserved and partitioned by date.  \n",
    "\n",
    "---\n",
    "\n",
    "✅ With this modular setup, analysts can quickly adjust **time windows, lookback period, field selection, or output tables** without modifying the core feature engineering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b287b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Date range for backfill\n",
    "# Time range (last 4 hours fresh data)\n",
    "run_end   = datetime.now().replace(minute=0, second=0, microsecond=0)\n",
    "run_start = run_end - timedelta(hours=4)\n",
    "lookback_days = 90\n",
    "\n",
    "# Workspace name (update for your environment)\n",
    "workspace_name = \"Woodgrove-LogAnalyiticsWorkspace\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table_raw  = \"SigninLogs\"\n",
    "input_table_summary = \"signin_summary_daily\"\n",
    "output_datalake_table_features = \"password_spray_features\" \n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserDisplayName\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment -natively available in SigninLogs\n",
    "    \"LocationDetails\",         # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\"\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"📅 Window Parameters\")\n",
    "print(f\"   Start → End: {run_start} → {run_end}\")\n",
    "print(f\"   Lookback:   {lookback_days} days\\n\")\n",
    "\n",
    "print(\"📂 Tables\")\n",
    "print(f\"   Input RAW:      {input_table_raw}\")\n",
    "print(f\"   Input Summary:      {input_table_summary}\")\n",
    "print(f\"   Candidates: {output_datalake_table_features}\\n\")\n",
    "\n",
    "print(\"📝 Selected Fields:\")\n",
    "print(\"   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c4b92",
   "metadata": {},
   "source": [
    "## 📥 Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Azure AD `SigninLogs` table.  \n",
    "\n",
    "- 📌 **Purpose**: Focus only on fields relevant to password spray detection.  \n",
    "- 🌍 **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).  \n",
    "- 🔢 **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.  \n",
    "- 📅 **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.  \n",
    "\n",
    "✅ At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recent_raw = (\n",
    "    data_provider.read_table(input_table_raw, workspace_name)\n",
    "    .select(*signin_fields)\n",
    "    .filter((F.col(\"TimeGenerated\") >= F.lit(run_start)) &\n",
    "            (F.col(\"TimeGenerated\") <  F.lit(run_end)))\n",
    "    .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "    .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "    .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "    .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "    .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "    .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    ")\n",
    "\n",
    "df_recent_raw.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"✅ Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ee0a7",
   "metadata": {},
   "source": [
    "## 🔄 Recurring Features Run\n",
    "\n",
    "This notebook is designed to run on a **recurring schedule** (e.g., every 4 hours).  \n",
    "Each run produces **per-IP spray features** using a combination of **fresh raw SigninLogs** and a **historical summary table**.\n",
    "\n",
    "### Step-by-Step\n",
    "1. 🕒 **Define batch window**  \n",
    "   - `run_start` and `run_end` are aligned to the nearest hour (e.g., 00:00–04:00, 04:00–08:00).  \n",
    "   - Ensures consistent processing windows regardless of runtime.  \n",
    "\n",
    "2. 📥 **Load fresh SigninLogs**  \n",
    "   - Extract the last 4 hours of raw events.  \n",
    "   - Expand JSON location details (City, Country, Latitude, Longitude).  \n",
    "   - Aggregate attempts, successes, distinct users, and entropy.  \n",
    "\n",
    "3. 📅 **Load historical summary**  \n",
    "   - Pull N days of `signin_summary_daily`.  \n",
    "   - Provides context for **low-and-slow spray attempts** beyond the current batch window.  \n",
    "\n",
    "4. 🔗 **Combine fresh + history**  \n",
    "   - Union recent aggregates with historical rollups.  \n",
    "   - Ensures both short-term spikes and long-term campaigns are visible.  \n",
    "\n",
    "5. 🧮 **Compute features**  \n",
    "   - Aggregate per IP across the combined window.  \n",
    "   - Normalize metrics:  \n",
    "     - `distinct_users_norm`  \n",
    "     - `success_rate`  \n",
    "     - `entropy_norm`  \n",
    "   - Calculate a **spray_score** using weighted formula.  \n",
    "   - Assign categorical **labels** (LOW / MEDIUM / HIGH).  \n",
    "\n",
    "6. 💾 **Append to `password_spray_features`**  \n",
    "   - Results are written partitioned by `run_date` (aligned to `run_end`).  \n",
    "   - Enables dashboards, detections, and investigations to consume the latest spray features.  \n",
    "\n",
    "---\n",
    "\n",
    "📌 Each scheduled run enriches the `password_spray_features` table with **up-to-date per-IP signals**, helping analysts detect both **fast bursts** and **stealthy campaigns**.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph FeaturesRun[Recurring Features Run - Every 4 Hours]\n",
    "        A[🕒 Define run_start & run_end<br/>Aligned to Hour] --> B[📥 Load Fresh Raw SigninLogs<br/>Expand Geo + ASN]\n",
    "        B --> C[🧮 Aggregate per IP<br/>Attempts, Successes, Distinct Users, Entropy]\n",
    "        A --> D[📅 Load signin_summary_daily<br/>Last N Days Context]\n",
    "        C --> E[🔗 Union Fresh + Historical]\n",
    "        D --> E\n",
    "        E --> F[📊 Compute Features<br/>Normalized Metrics]\n",
    "        F --> G[🎯 Spray Score + Label]\n",
    "        G --> H[💾 Write to password_spray_features<br/>Partition run_date]\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------\n",
    "# Candidate features (single groupBy)\n",
    "# Carry over ASN, City, Country for attribution\n",
    "# -----------------\n",
    "agg_recent = (\n",
    "    df_recent_raw.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\",\"date\")\n",
    "      .agg(\n",
    "        F.count(\"*\").alias(\"attempts_total\"),\n",
    "        F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "        F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "        F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "        F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# Entropy (optimized with window instead of join)\n",
    "# -----------------\n",
    "user_counts = df_recent_raw.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "entropy_recent = (\n",
    "    user_counts\n",
    "        .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "        .groupBy(\"IPAddress\")\n",
    "        .agg((-F.sum(F.col(\"p\")*F.log2(\"p\"))).alias(\"username_entropy\"))\n",
    ")\n",
    "\n",
    "df_recent_summary = agg_recent.join(entropy_recent, \"IPAddress\", \"left\")\n",
    "\n",
    "print(\"✅ Transformed recent logs into summary schema\")\n",
    "\n",
    "history_start = (run_end.date() - timedelta(days=lookback_days))\n",
    "\n",
    "df_history = (\n",
    "    data_provider.read_table(input_table_summary, workspace_name)\n",
    "      .filter(F.col(\"date\") >= F.lit(history_start.isoformat()))\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded historical summary from {history_start} → {run_end.date()}\")\n",
    "\n",
    "history_start = (run_end.date() - timedelta(days=lookback_days))\n",
    "\n",
    "df_combined = df_history.unionByName(df_recent_summary, allowMissingColumns=True)\n",
    "print(f\"✅ Combined {df_combined.count()} summary rows (history + fresh)\")\n",
    "\n",
    "\n",
    "features = (\n",
    "    df_recent_summary.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\")\n",
    "        .agg(\n",
    "            F.sum(\"attempts_total\").alias(\"attempts_total\"),\n",
    "            F.sum(\"success_count\").alias(\"success_count\"),\n",
    "            F.sum(\"distinct_users\").alias(\"distinct_users\"),\n",
    "            F.countDistinct(\"date\").alias(\"days_active\"),\n",
    "            F.min(\"first_seen\").alias(\"first_seen\"),\n",
    "            F.max(\"last_seen\").alias(\"last_seen\"),\n",
    "            F.round(F.avg(\"username_entropy\"), 2).alias(\"avg_entropy\")\n",
    "        )\n",
    "        .withColumn(\"run_date\", F.lit(run_end.date().isoformat()))\n",
    "        .withColumn(\"detection_window_start\", F.lit(history_start.isoformat()))\n",
    "        .withColumn(\"detection_window_end\", F.lit(run_end.date().isoformat()))\n",
    ")\n",
    "\n",
    "# Normalize & spray score\n",
    "global_max = features.agg(\n",
    "    F.max(\"distinct_users\").alias(\"max_users\"),\n",
    "    F.max(\"avg_entropy\").alias(\"max_entropy\")\n",
    ").first()\n",
    "\n",
    "max_users = global_max.max_users or 1\n",
    "max_entropy = global_max.max_entropy or 1\n",
    "\n",
    "features = (\n",
    "    features\n",
    "      .withColumn(\"distinct_users_norm\", F.col(\"distinct_users\") / F.lit(max_users))\n",
    "      .withColumn(\"success_rate\", F.col(\"success_count\") / F.greatest(F.col(\"attempts_total\"), F.lit(1)))\n",
    "      .withColumn(\"entropy_norm\", F.col(\"avg_entropy\") / F.lit(max_entropy))\n",
    "      .withColumn(\n",
    "            \"spray_score\",\n",
    "            F.round(\n",
    "                0.5*F.col(\"distinct_users_norm\") +\n",
    "                0.2*(1 - F.col(\"success_rate\")) +\n",
    "                0.3*F.col(\"entropy_norm\"),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "      .withColumn(\n",
    "          \"spray_score_label\",\n",
    "          F.when(F.col(\"spray_score\") < 0.3, \"LOW\")\n",
    "           .when(F.col(\"spray_score\") < 0.6, \"MEDIUM\")\n",
    "           .otherwise(\"HIGH\")\n",
    "      )\n",
    ")\n",
    "\n",
    "print(\"✅ Computed spray score & labels\")\n",
    "\n",
    "\n",
    "# Write candidates\n",
    "data_provider.save_as_table(features, output_datalake_table_features, write_options=write_options)\n",
    "print(f\"🎯 Wrote spray candidates for run {run_end} into {output_datalake_table_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcc3f1",
   "metadata": {},
   "source": [
    "# 👀 Preview Outputs\n",
    "\n",
    "After each scheduled run, it’s important to validate that the pipeline produced the expected data.  \n",
    "This section shows the **schema** and **sample rows** for the candidates table.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Candidates Table (`password_spray_features`)\n",
    "\n",
    "This table contains **per-IP aggregated features** across the rolling lookback window.  \n",
    "It highlights which IP addresses exhibit password spray-like behavior and provides the necessary context for triage.\n",
    "\n",
    "- 🌍 **Geo & ASN context** → IPAddress, ASN, City, Country  \n",
    "- 📊 **Behavioral metrics** → attempts_total, distinct_users, days_active, entropy  \n",
    "- 🔢 **Normalized features** → distinct_users_norm, entropy_norm, success_rate  \n",
    "- 🎯 **Spray score** → weighted score reflecting spray likelihood  \n",
    "- 🏷️ **Spray score label** → LOW / MEDIUM / HIGH for easier prioritization  \n",
    "\n",
    "---\n",
    "\n",
    "These outputs enable:\n",
    "- 📈 **Dashboards** (geo heatmaps, spray score trends, IP hotspots)  \n",
    "- 🚨 **Alerts** (triggering on high-score IPs)  \n",
    "- 🕵️ **Investigations** (pivoting into ASN, city, or recurring IP patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📑 Spray Candidates Schema\")\n",
    "features.printSchema()\n",
    "\n",
    "print(\"\\n🔍 Sample Rows\")\n",
    "display(\n",
    "    features.select(\n",
    "        \"IPAddress\", \"ASN\", \"City\", \"Country\",\n",
    "        \"attempts_total\", \"distinct_users\", \"avg_entropy\",\n",
    "        \"spray_score\", \"spray_score_label\"\n",
    "    ).limit(20)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [02_password_spray_features-4hour]",
   "language": "Python",
   "name": "MSGMedium"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
