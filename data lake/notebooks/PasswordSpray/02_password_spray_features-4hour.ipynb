{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8647baf6",
   "metadata": {},
   "source": [
    "# ğŸ” Password Spray Features Notebook\n",
    "\n",
    "## ğŸ“– Overview\n",
    "This notebook generates the **Password Spray Features Table** by combining recent raw `SigninLogs` with historical rollups from `signin_summary_daily`.  \n",
    "It enriches per-IP behavior with normalized metrics, a composite spray score, and categorical labels for triage.  \n",
    "\n",
    "This design allows efficient detection of:\n",
    "- ğŸš¨ **High-volume sprays** (short bursts visible in raw logs)  \n",
    "- ğŸ¢ **Low-and-slow sprays** (persistent activity captured across 30-day lookbacks via summary table)  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "- âœ… Load recent raw `SigninLogs` and expand JSON location details into structured fields (City, Country, ASN).  \n",
    "- âœ… Load historical `signin_summary_daily_SPRK` for N-day lookback without rescanning raw logs.  \n",
    "- âœ… Compute per-IP aggregated features: attempts, successes, distinct users, days active, entropy.  \n",
    "- âœ… Normalize features and calculate a **spray_score** using weighted components:  \n",
    "  - `distinct_users_norm` â†’ breadth of targeted accounts  \n",
    "  - `success_rate` â†’ effectiveness of attempts  \n",
    "  - `entropy_norm` â†’ randomness/distribution of usernames  \n",
    "- âœ… Assign categorical labels (**LOW / MEDIUM / HIGH**) based on thresholds.  \n",
    "- âœ… Write enriched feature rows into `password_spray_features_SPRK`, partitioned by run date.  \n",
    "- âœ… Provide schema + sample preview for validation.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Workflow\n",
    "1. ğŸ”§ **Parameters & Config** â€” define lookback period, table names, selected fields.  \n",
    "2. ğŸ“¥ **Load Recent Raw Logs** â€” ingest `SigninLogs` for the last few hours/day, expand location JSON into structured columns.  \n",
    "3. ğŸ§® **Aggregate Recent Data** â€” compute per-IP counts (attempts, distinct users, successes, first/last seen) and entropy.  \n",
    "4. ğŸ“… **Load Historical Summary** â€” pull per-IP daily rollups from `signin_summary_daily_SPRK` covering the lookback window.  \n",
    "5. ğŸ”— **Combine History + Recent** â€” merge both datasets into a unified summary.  \n",
    "6. ğŸ“Š **Generate Features** â€” aggregate across the merged window to produce normalized features.  \n",
    "7. ğŸ¯ **Compute Spray Score & Labels** â€” apply formula and assign LOW/MEDIUM/HIGH classification.  \n",
    "8. ğŸ’¾ **Write to Features Table** â€” save results into `password_spray_features_SPRK`, partitioned by run_date.  \n",
    "9. ğŸ‘€ **Preview Outputs** â€” inspect schema and sample rows for validation.  \n",
    "10. âš ï¸ **Optional Reset** â€” commented safeguard to delete table if re-run is required.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Data Flow (Mermaid Diagram)\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% Inputs\n",
    "    A[ğŸ“¥ SigninLogs Raw Data - Recent] --> A1[ğŸ“‘ Select Columns<br/>User, IP, ASN, Location, Result*]\n",
    "    B[ğŸ’¾ signin_summary_daily_SPRK Historical Rollups] --> B1[ğŸ“… Filter Last N Days<br/>lookback window]\n",
    "\n",
    "    %% Recent Processing\n",
    "    A1 --> C[ğŸ§® Aggregate per IP Recent<br/>attempts_total, success_count,<br/>distinct_users, first_seen, last_seen]\n",
    "    A1 --> D[ğŸ§® Username Entropy Recent<br/>calc p=count/sum, -Î£ p*log2 p]\n",
    "    C --> E[ğŸ”— Join Aggregates and Entropy]\n",
    "    D --> E\n",
    "\n",
    "    %% Combine with History\n",
    "    B1 --> F[ğŸ”— Union Recent Summary and Historical Summary]\n",
    "    E --> F\n",
    "\n",
    "    %% Feature Aggregates\n",
    "    F --> G[ğŸ“Š Group per IP<br/>Î£ attempts_total, Î£ success_count,<br/>Î£ distinct_users, count days_active,<br/>avg entropy, first_seen, last_seen]\n",
    "\n",
    "    %% Normalization & Scoring\n",
    "    G --> H[âš–ï¸ Normalize Features<br/>distinct_users_norm, success_rate,<br/>entropy_norm]\n",
    "    H --> I[ğŸ¯ Compute Spray Score<br/>0.5*users_norm + 0.2* 1-success_rate<br/>+ 0.3*entropy_norm]\n",
    "    I --> J[ğŸ·ï¸ Assign Label by Score<br/>LOW <0.3, MEDIUM <0.6, HIGH]\n",
    "\n",
    "    %% Output\n",
    "    J --> K[ğŸ’¾ password_spray_features<br/>partitioned by run_date]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a8d0a",
   "metadata": {},
   "source": [
    "# ğŸ”§ Parameters & Config\n",
    "\n",
    "This section defines the **runtime parameters** and ensures the notebook can be reused or adapted easily.  \n",
    "It sets up **time windows, input/output tables, and selected fields** before processing begins.\n",
    "\n",
    "- â±ï¸ **Time ranges**  \n",
    "  - `run_start` â†’ start of the current processing window (last 4 hours by default).  \n",
    "  - `run_end` â†’ end of the current processing window.  \n",
    "  - `lookback_days` â†’ number of historical days pulled from `signin_summary_daily` to combine with fresh data.  \n",
    "\n",
    "- ğŸ—‚ï¸ **Table names**  \n",
    "  - Raw input â†’ `SigninLogs` (fresh, high-fidelity events).  \n",
    "  - Historical summary â†’ `signin_summary_daily` (compact daily rollups).  \n",
    "  - Output â†’ `password_spray_features` (normalized features, spray score, and labels).  \n",
    "\n",
    "- ğŸ“ **Selected fields**  \n",
    "  - Core identity fields â†’ `UserPrincipalName`, `UserDisplayName`, `UserType`.  \n",
    "  - Request context â†’ `IPAddress`, `ResultType`, `ResultSignature`, `Status`, `UserAgent`.  \n",
    "  - Enrichment fields â†’ `AutonomousSystemNumber`, `LocationDetails` (JSON expanded into City, Country, Latitude, Longitude).  \n",
    "\n",
    "- âš™ï¸ **Write options**  \n",
    "  - Append mode is used, so historical runs are preserved and partitioned by date.  \n",
    "\n",
    "---\n",
    "\n",
    "âœ… With this modular setup, analysts can quickly adjust **time windows, lookback period, field selection, or output tables** without modifying the core feature engineering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b287b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "\n",
    "# Date range for backfill\n",
    "# Time range (last 4 hours fresh data)\n",
    "run_end   = datetime.now().replace(minute=0, second=0, microsecond=0)\n",
    "run_start = run_end - timedelta(hours=4)\n",
    "lookback_days = 90\n",
    "\n",
    "# Sentinel Workspace name (update for your environment)\n",
    "workspace_name = \"<YOUR_WORKSPACE_NAME>\"  # Replace with your actual workspace name\n",
    "\n",
    "# Table names (easy to swap)\n",
    "input_table_raw  = \"SigninLogs\"\n",
    "input_table_summary = \"signin_summary_daily_SPRK\"\n",
    "output_datalake_table_features = \"password_spray_features_SPRK\" \n",
    "\n",
    "# Write options (append mode keeps history)\n",
    "write_options = {\"mode\": \"append\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Field selection\n",
    "# -----------------------------\n",
    "# Core fields + native enrichment (ASN, geolocation)\n",
    "signin_fields = [\n",
    "    \"TimeGenerated\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"UserDisplayName\",\n",
    "    \"IPAddress\",\n",
    "    \"ResultType\",\n",
    "    \"ResultSignature\",\n",
    "    \"Status\",\n",
    "    \"UserDisplayName\",\n",
    "    \"UserType\",\n",
    "    \"UserAgent\",\n",
    "    # Geo & ASN enrichment -natively available in SigninLogs\n",
    "    \"LocationDetails\",         # JSON object containing detailed location info - city, state, countryorRegion, geoCoordinates\n",
    "    \"AutonomousSystemNumber\"\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Status output\n",
    "# -----------------------------\n",
    "print(\"ğŸ“… Window Parameters\")\n",
    "print(f\"   Start â†’ End: {run_start} â†’ {run_end}\")\n",
    "print(f\"   Lookback:   {lookback_days} days\\n\")\n",
    "\n",
    "print(\"ğŸ“‚ Tables\")\n",
    "print(f\"   Input RAW:      {input_table_raw}\")\n",
    "print(f\"   Input Summary:      {input_table_summary}\")\n",
    "print(f\"   Candidates: {output_datalake_table_features}\\n\")\n",
    "\n",
    "print(\"ğŸ“ Selected Fields:\")\n",
    "print(\"   \" + \", \".join(signin_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c4b92",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Load Raw SigninLogs\n",
    "\n",
    "We read the source authentication data from the Microsoft Entra AD `SigninLogs` table.  \n",
    "\n",
    "- ğŸ“Œ **Purpose**: Focus only on fields relevant to password spray detection.  \n",
    "- ğŸŒ **Enrichment**: Expand the `LocationDetails` JSON into flat columns (City, State, Country, Latitude, Longitude).  \n",
    "- ğŸ”¢ **Network context**: Capture `AutonomousSystemNumber (ASN)` for attribution to ISPs and hosting providers.  \n",
    "- ğŸ“… **Date column**: Add a derived `date` column (from `TimeGenerated`) to support partition pruning and time-based queries.  \n",
    "\n",
    "âœ… At this stage, the data is **normalized and structured**, ready for feature engineering in the backfill loop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recent_raw = (\n",
    "    data_provider.read_table(input_table_raw, workspace_name)\n",
    "    .select(*signin_fields)\n",
    "    .filter((F.col(\"TimeGenerated\") >= F.lit(run_start)) &\n",
    "            (F.col(\"TimeGenerated\") <  F.lit(run_end)))\n",
    "    .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
    "    .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
    "    .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
    "    .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
    "    .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
    "    .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
    ")\n",
    "\n",
    "df_recent_raw.printSchema()  # Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
    "print(\"âœ… Loaded SigninLogs with expanded LocationDetails and ASN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ee0a7",
   "metadata": {},
   "source": [
    "## ğŸ”„ Recurring Features Run\n",
    "\n",
    "This notebook is designed to run on a **recurring schedule** (e.g., every 4 hours).  \n",
    "Each run produces **per-IP spray features** using a combination of **fresh raw SigninLogs** and a **historical summary table**.\n",
    "\n",
    "### Step-by-Step\n",
    "1. ğŸ•’ **Define batch window**  \n",
    "   - `run_start` and `run_end` are aligned to the nearest hour (e.g., 00:00â€“04:00, 04:00â€“08:00).  \n",
    "   - Ensures consistent processing windows regardless of runtime.  \n",
    "\n",
    "2. ğŸ“¥ **Load fresh SigninLogs**  \n",
    "   - Extract the last 4 hours of raw events.  \n",
    "   - Expand JSON location details (City, Country, Latitude, Longitude).  \n",
    "   - Aggregate attempts, successes, distinct users, and entropy.  \n",
    "\n",
    "3. ğŸ“… **Load historical summary**  \n",
    "   - Pull N days of `signin_summary_daily_SPRK`.  \n",
    "   - Provides context for **low-and-slow spray attempts** beyond the current batch window.  \n",
    "\n",
    "4. ğŸ”— **Combine fresh + history**  \n",
    "   - Union recent aggregates with historical rollups.  \n",
    "   - Ensures both short-term spikes and long-term campaigns are visible.  \n",
    "\n",
    "5. ğŸ§® **Compute features**  \n",
    "   - Aggregate per IP across the combined window.  \n",
    "   - Normalize metrics:  \n",
    "     - `distinct_users_norm`  \n",
    "     - `success_rate`  \n",
    "     - `entropy_norm`  \n",
    "   - Calculate a **spray_score** using weighted formula.  \n",
    "   - Assign categorical **labels** (LOW / MEDIUM / HIGH).  \n",
    "\n",
    "6. ğŸ’¾ **Append to `password_spray_features_SPRK`**  \n",
    "   - Results are written partitioned by `run_date` (aligned to `run_end`).  \n",
    "   - Enables dashboards, detections, and investigations to consume the latest spray features.  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“Œ Each scheduled run enriches the `password_spray_features_SPRK` table with **up-to-date per-IP signals**, helping analysts detect both **fast bursts** and **stealthy campaigns**.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph FeaturesRun[Recurring Features Run - Every 4 Hours]\n",
    "        A[ğŸ•’ Define run_start & run_end<br/>Aligned to Hour] --> B[ğŸ“¥ Load Fresh Raw SigninLogs<br/>Expand Geo + ASN]\n",
    "        B --> C[ğŸ§® Aggregate per IP<br/>Attempts, Successes, Distinct Users, Entropy]\n",
    "        A --> D[ğŸ“… Load signin_summary_daily_SPRK<br/>Last N Days Context]\n",
    "        C --> E[ğŸ”— Union Fresh + Historical]\n",
    "        D --> E\n",
    "        E --> F[ğŸ“Š Compute Features<br/>Normalized Metrics]\n",
    "        F --> G[ğŸ¯ Spray Score + Label]\n",
    "        G --> H[ğŸ’¾ Write to password_spray_features_SPRK<br/>Partition run_date]\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------\n",
    "# Candidate features (single groupBy)\n",
    "# Carry over ASN, City, Country for attribution\n",
    "# -----------------\n",
    "agg_recent = (\n",
    "    df_recent_raw.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\",\"date\")\n",
    "      .agg(\n",
    "        F.count(\"*\").alias(\"attempts_total\"),\n",
    "        F.sum(F.when(F.col(\"ResultSignature\")==\"Success\",1).otherwise(0)).alias(\"success_count\"),\n",
    "        F.countDistinct(\"UserPrincipalName\").alias(\"distinct_users\"),\n",
    "        F.min(\"TimeGenerated\").alias(\"first_seen\"),\n",
    "        F.max(\"TimeGenerated\").alias(\"last_seen\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# Entropy (optimized with window instead of join)\n",
    "# -----------------\n",
    "user_counts = df_recent_raw.groupBy(\"IPAddress\",\"UserPrincipalName\").count()\n",
    "entropy_recent = (\n",
    "    user_counts\n",
    "        .withColumn(\"p\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"IPAddress\")))\n",
    "        .groupBy(\"IPAddress\")\n",
    "        .agg((-F.sum(F.col(\"p\")*F.log2(\"p\"))).alias(\"username_entropy\"))\n",
    ")\n",
    "\n",
    "df_recent_summary = agg_recent.join(entropy_recent, \"IPAddress\", \"left\")\n",
    "\n",
    "print(\"âœ… Transformed recent logs into summary schema\")\n",
    "\n",
    "history_start = (run_end.date() - timedelta(days=lookback_days))\n",
    "\n",
    "df_history = (\n",
    "    data_provider.read_table(input_table_summary, workspace_name)\n",
    "      .filter(F.col(\"date\") >= F.lit(history_start.isoformat()))\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded historical summary from {history_start} â†’ {run_end.date()}\")\n",
    "\n",
    "history_start = (run_end.date() - timedelta(days=lookback_days))\n",
    "\n",
    "df_combined = df_history.unionByName(df_recent_summary, allowMissingColumns=True)\n",
    "print(f\"âœ… Combined {df_combined.count()} summary rows (history + fresh)\")\n",
    "\n",
    "\n",
    "features = (\n",
    "    df_recent_summary.groupBy(\"IPAddress\", \"ASN\", \"City\", \"Country\")\n",
    "        .agg(\n",
    "            F.sum(\"attempts_total\").alias(\"attempts_total\"),\n",
    "            F.sum(\"success_count\").alias(\"success_count\"),\n",
    "            F.sum(\"distinct_users\").alias(\"distinct_users\"),\n",
    "            F.countDistinct(\"date\").alias(\"days_active\"),\n",
    "            F.min(\"first_seen\").alias(\"first_seen\"),\n",
    "            F.max(\"last_seen\").alias(\"last_seen\"),\n",
    "            F.round(F.avg(\"username_entropy\"), 2).alias(\"avg_entropy\")\n",
    "        )\n",
    "        .withColumn(\"run_date\", F.lit(run_end.date().isoformat()))\n",
    "        .withColumn(\"detection_window_start\", F.lit(history_start.isoformat()))\n",
    "        .withColumn(\"detection_window_end\", F.lit(run_end.date().isoformat()))\n",
    ")\n",
    "\n",
    "# Normalize & spray score\n",
    "global_max = features.agg(\n",
    "    F.max(\"distinct_users\").alias(\"max_users\"),\n",
    "    F.max(\"avg_entropy\").alias(\"max_entropy\")\n",
    ").first()\n",
    "\n",
    "max_users = global_max.max_users or 1\n",
    "max_entropy = global_max.max_entropy or 1\n",
    "\n",
    "features = (\n",
    "    features\n",
    "      .withColumn(\"distinct_users_norm\", F.col(\"distinct_users\") / F.lit(max_users))\n",
    "      .withColumn(\"success_rate\", F.col(\"success_count\") / F.greatest(F.col(\"attempts_total\"), F.lit(1)))\n",
    "      .withColumn(\"entropy_norm\", F.col(\"avg_entropy\") / F.lit(max_entropy))\n",
    "      .withColumn(\n",
    "            \"spray_score\",\n",
    "            F.round(\n",
    "                0.5*F.col(\"distinct_users_norm\") +\n",
    "                0.2*(1 - F.col(\"success_rate\")) +\n",
    "                0.3*F.col(\"entropy_norm\"),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "      .withColumn(\n",
    "          \"spray_score_label\",\n",
    "          F.when(F.col(\"spray_score\") < 0.3, \"LOW\")\n",
    "           .when(F.col(\"spray_score\") < 0.6, \"MEDIUM\")\n",
    "           .otherwise(\"HIGH\")\n",
    "      )\n",
    ")\n",
    "\n",
    "print(\"âœ… Computed spray score & labels\")\n",
    "\n",
    "\n",
    "# Write candidates\n",
    "data_provider.save_as_table(features, output_datalake_table_features, write_options=write_options)\n",
    "print(f\"ğŸ¯ Wrote spray candidates for run {run_end} into {output_datalake_table_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcc3f1",
   "metadata": {},
   "source": [
    "# ğŸ‘€ Preview Outputs\n",
    "\n",
    "After each scheduled run, itâ€™s important to validate that the pipeline produced the expected data.  \n",
    "This section shows the **schema** and **sample rows** for the candidates table.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© Candidates Table (`password_spray_features_SPRK`)\n",
    "\n",
    "This table contains **per-IP aggregated features** across the rolling lookback window.  \n",
    "It highlights which IP addresses exhibit password spray-like behavior and provides the necessary context for triage.\n",
    "\n",
    "- ğŸŒ **Geo & ASN context** â†’ IPAddress, ASN, City, Country  \n",
    "- ğŸ“Š **Behavioral metrics** â†’ attempts_total, distinct_users, days_active, entropy  \n",
    "- ğŸ”¢ **Normalized features** â†’ distinct_users_norm, entropy_norm, success_rate  \n",
    "- ğŸ¯ **Spray score** â†’ weighted score reflecting spray likelihood  \n",
    "- ğŸ·ï¸ **Spray score label** â†’ LOW / MEDIUM / HIGH for easier prioritization  \n",
    "\n",
    "---\n",
    "\n",
    "These outputs enable:\n",
    "- ğŸ“ˆ **Dashboards** (geo heatmaps, spray score trends, IP hotspots)  \n",
    "- ğŸš¨ **Alerts** (triggering on high-score IPs)  \n",
    "- ğŸ•µï¸ **Investigations** (pivoting into ASN, city, or recurring IP patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‘ Spray Candidates Schema\")\n",
    "features.printSchema()\n",
    "\n",
    "print(\"\\nğŸ” Sample Rows\")\n",
    "display(\n",
    "    features.select(\n",
    "        \"IPAddress\", \"ASN\", \"City\", \"Country\",\n",
    "        \"attempts_total\", \"distinct_users\", \"avg_entropy\",\n",
    "        \"spray_score\", \"spray_score_label\"\n",
    "    ).limit(20)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [02_password_spray_features-4hour]",
   "language": "Python",
   "name": "MSGMedium"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
