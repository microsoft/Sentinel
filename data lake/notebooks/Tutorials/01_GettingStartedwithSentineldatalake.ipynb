{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "56308008",
      "metadata": {},
      "source": [
        "# üöÄ Getting Started with Apache Spark for Microsoft Sentinel data lake\n",
        "\n",
        "This notebook is designed for **beginners** (security analysts, data engineers, SOC researchers) \n",
        "who are new to working with Spark notebooks and Sentinel data lake.\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you'll be able to:\n",
        "- üîó Connect to Sentinel data lake using the `MicrosoftSentinelProvider`\n",
        "- üìä Load and explore log data (SecurityEvent, SigninLogs, AuditLogs, etc.)\n",
        "- üîç Perform common **security analysis queries** using Spark operations\n",
        "- üíæ Save processed results back to the data lake for further analysis\n",
        "- üóëÔ∏è Safely manage tables (create, read, delete)\n",
        "\n",
        "> **üí° Tip**: This is a hands-on tutorial - run each cell step by step to see the results!\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Step 1: Connect to Sentinel data lake\n",
        "\n",
        "First, we'll establish a connection using the **MicrosoftSentinelProvider** - your gateway to Sentinel data lake.\n",
        "\n",
        "### What is MicrosoftSentinelProvider?\n",
        "The `MicrosoftSentinelProvider` is a Python class that:\n",
        "- üîó Connects your Spark session to Microsoft Sentinel data lake\n",
        "- üìã Lists available databases and tables\n",
        "- üìñ Reads security log data at scale\n",
        "- üíæ Saves processed results back to the data lake\n",
        "\n",
        "### Getting Started\n",
        "‚û°Ô∏è Simply initialize the provider with your active Spark session - that's it!\n",
        "\n",
        "üìö **Learn More**: [Microsoft Sentinel Provider Class Reference](https://learn.microsoft.com/en-us/azure/sentinel/datalake/sentinel-provider-class-reference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f919bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generic libraries for data manipulation and pyspark operations\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "# Loading the MicrosoftSentinelProvider from sentinel_lake library\n",
        "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
        "\n",
        "# Initialize provider\n",
        "data_provider = MicrosoftSentinelProvider(spark)\n",
        "\n",
        "print(\"‚úÖ MicrosoftSentinelProvider initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "335cb4a1",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 2: Configure Parameters & Tables\n",
        "\n",
        "Before we dive into the data, let's set up our configuration parameters.\n",
        "\n",
        "### üìã What We're Configuring:\n",
        "- **üïí Time Window** ‚Üí How far back to look for data (e.g., last 1 hour)\n",
        "- **üìÇ Input Table** ‚Üí Which Sentinel table to read from (SigninLogs in our example)\n",
        "- **üéØ Output Table** ‚Üí Where to save our processed results\n",
        "- **üè¢ Workspace** ‚Üí Your Sentinel workspace name\n",
        "\n",
        "### üîß Why This Matters:\n",
        "- **Reusability**: Change dates/tables without modifying code\n",
        "- **Performance**: Smaller time windows = faster queries\n",
        "- **Organization**: Clear naming helps track your analysis\n",
        "\n",
        "> **‚ö†Ô∏è Remember**: Replace `<YOUR_WORKSPACE_NAME>` with your actual Sentinel workspace name!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1ed9cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time window\n",
        "lookback_hours = 4  # Adjust Lookback period in hours to match with filter needs\n",
        "# Define run_start and run_end to be used in queries\n",
        "run_end = datetime.now().replace(minute=0, second=0, microsecond=0)  # Round down to the nearest hour\n",
        "run_start = run_end - timedelta(hours=lookback_hours)   # Setting start time based on lookback hours relative to run_end\n",
        "\n",
        "# Workspace name (replace with your own Sentinel workspace)\n",
        "workspace_name = \"<workspace_name>\"  # Change this to your Sentinel workspace name  \n",
        "\n",
        "# Table names\n",
        "input_table_raw = \"SigninLogs\"\n",
        "output_data_lake_table = \"Test_output_table_SPRK\"    # Chage this to your desired output table name - make sure to have _SPRK suffix\n",
        "\n",
        "# Write options (append keeps history, partition improves query performance)\n",
        "write_options = {\"mode\": \"append\"} # simple append mode options for demo\n",
        "\n",
        "print(\"üìÖ Time Window:\", run_start, \"‚Üí\", run_end)\n",
        "print(\"‚úÖ Parameters configured\")\n",
        "\n",
        "print(\"üìÇ Tables configured as:\")\n",
        "print(f\"\\t   Input:      {input_table_raw}\")\n",
        "print(f\"\\t   Output Table: {output_data_lake_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739443b6",
      "metadata": {},
      "source": [
        "## üìä Step 3: Load & Transform Data\n",
        "\n",
        "Now we'll load real security data from the **SigninLogs** table and prepare it for analysis.\n",
        "\n",
        "### üéØ What This Code Does:\n",
        "1. **üìñ Reads Data**: Connects to SigninLogs in your Analytics Tier workspace\n",
        "2. **üîç Filters Time**: Only gets data from our specified time window\n",
        "3. **üèóÔ∏è Transforms Data**: \n",
        "   - Selects key security fields (user, IP, location, etc.)\n",
        "   - Expands JSON location data into separate columns (City, Country, coordinates)\n",
        "   - Adds a date column for easier querying\n",
        "4. **‚ö° Optimizes**: Caches the data in memory for faster subsequent operations\n",
        "\n",
        "### üóÇÔ∏è Key Fields We're Working With:\n",
        "- **Identity**: UserPrincipalName, UserDisplayName\n",
        "- **Network**: IPAddress, AutonomousSystemNumber (ASN)\n",
        "- **Location**: City, Country, Latitude/Longitude (from LocationDetails JSON)\n",
        "- **Security**: ResultType, ResultSignature, Status\n",
        "- **Context**: UserAgent, TimeGenerated\n",
        "\n",
        "> **üí° Pro Tip**: The `.cache()` operation stores data in memory, making repeated queries much faster!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d5e0b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core fields + enrichment\n",
        "signin_fields = [\n",
        "    \"TimeGenerated\",\n",
        "    \"UserPrincipalName\",\n",
        "    \"UserDisplayName\",\n",
        "    \"IPAddress\",\n",
        "    \"ResultType\",\n",
        "    \"ResultSignature\",\n",
        "    \"Status\",\n",
        "    \"UserType\",\n",
        "    \"UserAgent\",\n",
        "    \"LocationDetails\",      # JSON: city, state, countryOrRegion, geoCoordinates - the columns are part of LocationDetails dynamic JSON field and will be expanded below\n",
        "    \"AutonomousSystemNumber\"\n",
        "]\n",
        "\n",
        "df_recent_raw = (\n",
        "    data_provider.read_table(input_table_raw, workspace_name)   # Read from SigninLogs table from Analytics tier - workspace_name is provided\n",
        "    .select(*signin_fields)\n",
        "    .filter((F.col(\"TimeGenerated\") > F.lit(run_start)) & \n",
        "            (F.col(\"TimeGenerated\") <= F.lit(run_end))) # Filter by time window as defined above\n",
        "    .withColumn(\"date\", F.to_date(\"TimeGenerated\"))\n",
        "    # Expand JSON fields\n",
        "    .withColumn(\"City\", F.get_json_object(\"LocationDetails\", \"$.city\"))\n",
        "    .withColumn(\"Country\", F.get_json_object(\"LocationDetails\", \"$.countryOrRegion\"))\n",
        "    .withColumn(\"Latitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.latitude\").cast(\"double\"))\n",
        "    .withColumn(\"Longitude\", F.get_json_object(\"LocationDetails\", \"$.geoCoordinates.longitude\").cast(\"double\"))\n",
        "    .withColumnRenamed(\"AutonomousSystemNumber\", \"ASN\")\n",
        "    .cache()\n",
        ")\n",
        "\n",
        "# Prints only the DataFrame schema (metadata), does not scan the data so lightweight\n",
        "df_recent_raw.printSchema()\n",
        "print(\"‚úÖ Loaded SigninLogs with expanded LocationDetails and ASN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe29ff8",
      "metadata": {},
      "source": [
        "## üîç Step 4: Explore Security Data\n",
        "\n",
        "Now for the fun part - let's analyze the data! We'll demonstrate common security analysis patterns.\n",
        "\n",
        "### üéØ Example Analysis: Failed Login Attempts\n",
        "\n",
        "We're looking for accounts with failed authentication attempts (ResultType = \"50126\").\n",
        "This helps identify:\n",
        "- üö® **Potential brute force attacks**\n",
        "- üîê **Accounts under attack**\n",
        "- üìä **Attack patterns and trends**\n",
        "\n",
        "### üõ†Ô∏è What the Code Does:\n",
        "1. **Filters** for failed login attempts\n",
        "2. **Groups** by user account\n",
        "3. **Counts** failures per account\n",
        "4. **Sorts** by highest failure count\n",
        "5. **Checks** if any results were found\n",
        "6. **Conditionally displays** results with appropriate messaging:\n",
        "   - If records found ‚Üí Shows HTML table with top 10 targeted accounts\n",
        "   - If no records ‚Üí Provides helpful context and next steps\n",
        "\n",
        "> **üí° Security Insight**: High failure counts on specific accounts often indicate targeted attacks or compromised credentials!\n",
        "\n",
        "### üîß **Troubleshooting Display Issues:**\n",
        "If you see `SynapseWidget(Synapse.DataFrame, xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)` instead of an HTML table:\n",
        "1. **Click the 3 dots (‚ãØ)** next to the widget output cell with error\n",
        "2. **Select \"Change Presentation\"** from the dropdown menu\n",
        "3. **Choose a different renderer** (try \"VS Code Builtin Notebook Output Rendere\" or \"Fabric Data Engineering Notebook Output Renderer\")\n",
        "4. This will convert the widget to a proper interactive table view\n",
        "5. If it still does not show, you may have to restart the session.\n",
        "\n",
        "> **üí° Pro Tip**: This widget issue sometimes occurs in VS Code with certain DataFrame sizes or configurations, but the data is still there!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22796d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the analysis DataFrame\n",
        "failed_logins_df = (df_recent_raw.filter(F.col(\"ResultType\") == \"50126\") \n",
        "                   .groupBy(\"UserPrincipalName\")\n",
        "                   .count()\n",
        "                   .orderBy(F.desc(\"count\"))\n",
        "                   .limit(10))\n",
        "\n",
        "# Check if we have any results\n",
        "record_count = failed_logins_df.count()\n",
        "\n",
        "if record_count > 0:\n",
        "    print(f\"üìä Found {record_count} accounts with failed login attempts\")\n",
        "    print(\"‚è≥ Preparing to show DataFrame...Groupby-Count-Orderby Operations may take few minutes ‚åõ\")\n",
        "    display(failed_logins_df)\n",
        "    print(\"‚úÖ Displayed: Top accounts with failed logons\")\n",
        "else:\n",
        "    print(\"üéâ Great news! No failed login attempts found in the selected time window.\")\n",
        "    print(\"üí° This could mean:\")\n",
        "    print(\"   ‚Ä¢ Your environment is secure with no brute force attempts\")\n",
        "    print(\"   ‚Ä¢ The time window might be too narrow (try increasing lookback_hours)\")\n",
        "    print(\"   ‚Ä¢ Different ResultType codes might be relevant for your environment\")\n",
        "    print(\"   ‚Ä¢ Consider checking other security events like successful logins from new locations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3de51c8",
      "metadata": {},
      "source": [
        "## üíæ Step 5: Save Results to data lake\n",
        "\n",
        "After processing and analyzing data, you'll often want to save results for:\n",
        "- üìä **Dashboards and reports**\n",
        "- üîç **Future investigations** \n",
        "- ü§ù **Sharing with team members**\n",
        "- ‚ö° **Faster subsequent queries**\n",
        "\n",
        "### üéØ What We're Doing:\n",
        "Using `save_as_table()` to write our processed data back to the Sentinel data lake.\n",
        "\n",
        "### üìã Save Options:\n",
        "- **Table Name**: `Test_output_table_SPRK` (clearly marked as test data)\n",
        "- **Mode**: `append` (adds to existing data rather than overwriting)\n",
        "- **Tier**: \"System tables\" (data lake tier for long-term storage)\n",
        "\n",
        "> **‚ö†Ô∏è Important**: This creates a test table that we'll clean up at the end of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4910085",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    data_provider.save_as_table(\n",
        "            df_recent_raw,\n",
        "            output_data_lake_table,\n",
        "            \"System tables\",        # System tables refers to writing to data lake tier table.\n",
        "            write_options\n",
        "        )\n",
        "    print(f\"‚úÖ Wrote test data into {output_data_lake_table}\")\n",
        "except Exception as save_err:\n",
        "    print(f\"‚ùå Failed writing data into {output_data_lake_table}: {save_err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c429921",
      "metadata": {},
      "source": [
        "## üóëÔ∏è Step 6: Clean Up Test Data\n",
        "\n",
        "### ‚ö†Ô∏è **CAUTION: Table Deletion**\n",
        "\n",
        "We're about to demonstrate the `delete_table()` operation. This is **permanent** and **cannot be undone**.\n",
        "\n",
        "### üéØ Why We're Doing This:\n",
        "- üßπ **Clean up**: Remove the test table we created\n",
        "- üìö **Learning**: Show you how to safely manage tables\n",
        "- üí∞ **Cost control**: Avoid unnecessary storage charges\n",
        "\n",
        "### üõ°Ô∏è Safety Guidelines:\n",
        "- ‚úÖ **Only delete tables you created for testing**\n",
        "- ‚ùå **Never delete production tables without team approval**\n",
        "- üìù **Always double-check the table name before deletion**\n",
        "\n",
        "> **üè≠ Best Practice**: In production, implement approval workflows and backup procedures before any deletion operations!\n",
        "\n",
        "### üîç What's Being Deleted:\n",
        "Table: `Test_output_table_SPRK` (our demo table from Step 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86531f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"‚ö†Ô∏è Deleting table - {output_data_lake_table} - created for demo purposes\")\n",
        "data_provider.delete_table(output_data_lake_table) \n",
        "\n",
        "print(\"‚úÖ Specified Table deleted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fdbde2",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully completed your first Spark notebook for Microsoft Sentinel data lake analysis! \n",
        "\n",
        "### üöÄ What You've Accomplished:\n",
        "- ‚úÖ Connected to Sentinel data lake using MicrosoftSentinelProvider\n",
        "- ‚úÖ Loaded and transformed real security data (SigninLogs)\n",
        "- ‚úÖ Performed security analysis (failed login detection)\n",
        "- ‚úÖ Saved results back to the data lake\n",
        "- ‚úÖ Safely managed table operations (create/delete)\n",
        "\n",
        "### üéØ Next Steps:\n",
        "- **Explore More Tables**: Try SecurityEvent, AuditLogs, or DeviceEvents\n",
        "- **Advanced Analytics**: Implement time-series analysis and anomaly detection\n",
        "- **Automation**: Schedule notebooks to run automatically\n",
        "- **Visualization**: Create dashboards from your saved results\n",
        "\n",
        "---\n",
        "\n",
        "## üìö References & Further Learning\n",
        "\n",
        "### üîó Microsoft Sentinel data lake Documentation:\n",
        "- **[Sentinel Provider Class Reference](https://learn.microsoft.com/en-us/azure/sentinel/datalake/sentinel-provider-class-reference)** - Complete API documentation for MicrosoftSentinelProvider\n",
        "- **[Notebook Jobs in Sentinel](https://learn.microsoft.com/en-us/azure/sentinel/datalake/notebook-jobs)** - Learn to schedule and automate your notebooks\n",
        "\n",
        "### üìñ Additional Learning Resources:\n",
        "- **[Apache Spark Official Documentation](https://spark.apache.org/docs/latest/)** - Comprehensive Spark programming guide\n",
        "- **[PySpark Quickstart: DataFrame](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)**  - Introduction and quickstart for the PySpark DataFrame API\n",
        "- **[Microsoft Sentinel Skill-up Training](https://learn.microsoft.com/en-us/azure/sentinel/skill-up-resources)** - Free training modules for security analysts\n",
        "\n",
        "### üõ†Ô∏è Advanced Topics:\n",
        "- **[PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)** - Master data manipulation techniques\n",
        "- **[Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)** - Powerful built-in functions for data analysis\n",
        "\n",
        "### ü§ù Community & Support:\n",
        "- **[Microsoft Sentinel data lake GitHub](https://github.com/microsoft/Sentinel)** - Out-of-the Box Notebooks and KQL queries for data lake.\n",
        "- **[Microsoft Tech Community](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)** - Connect with other security professionals\n",
        "\n",
        "---\n",
        "\n",
        "> **üí° Pro Tip**: Use this notebook as a starter template for your own security analysis projects!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "medium pool (8 vCores) [01_GettingStartedwithSentineldatalake]",
      "language": "Python",
      "name": "MSGMedium"
    },
    "language_info": {
      "codemirror_mode": "ipython",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
